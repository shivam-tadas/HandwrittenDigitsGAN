# Handwritten Digits GAN

This repository contains a PyTorch implementation of a Generative Adversarial Network (GAN) designed to generate handwritten digits resembling those in a custom dataset. The code leverages CUDA for accelerated training on compatible GPUs.

## Overview

The GAN consists of:
- **Generator**: Creates synthetic 28x28 grayscale images from random noise.
- **Discriminator**: Distinguishes real images from generated ones.
- **Custom Dataset Loader**: Reads 28x28 PNG images with transparent backgrounds and black ink from a specified directory structure.

The model is trained to generate realistic handwritten digits, with periodic image outputs saved during training for progress visualization.

## Dataset

The code expects a dataset of handwritten digits stored at `D:\HandwrittenDigitsDataset\dataset` on your local machine. The directory structure should be:

```
D:\HandwrittenDigitsDataset\dataset
├── 0\0\1.png
├── 0\0\2.png
├── ...
├── 0\0\10772.png
├── 1\1\1.png
├── ...
├── 9\9\10772.png
```

Each PNG is a 28x28 grayscale image with a transparent background and black ink, similar to MNIST but in a custom format. The dataset is downloaded from Kaggle.

## Requirements

- Python 3.8+
- PyTorch (with CUDA support for GPU acceleration)
- Pillow (PIL)
- Matplotlib
- NumPy

Install dependencies using:

```bash
pip install torch torchvision pillow matplotlib numpy
```

Ensure you have a CUDA-compatible GPU and the appropriate CUDA toolkit installed for GPU acceleration.

## Usage

1. **Clone the Repository**:

   ```bash
   git clone https://github.com/your-username/handwritten-digits-gan.git
   cd handwritten-digits-gan
   ```

2. **Prepare the Dataset**:

   Place your dataset in `D:\HandwrittenDigitsDataset\dataset` with the structure described above. Update the path in `gan_handwritten_digits.py` if your dataset is located elsewhere.

3. **Run the Training**:

   Execute the script to train the GAN:

   ```bash
   python gan_handwritten_digits.py
   ```

   - Training runs for 50 epochs by default.
   - Every 10 epochs, generated sample images are saved as `generated_digits_epoch_X.png`.
   - Model checkpoints are saved as `generator.pth` and `discriminator.pth`.

4. **Monitor Progress**:

   - Training progress is printed every 100 batches, showing discriminator and generator losses.
   - Check the generated images in the project directory to evaluate quality.

## Code Structure

- **gan_handwritten_digits.py**: Main script containing:
  - Custom `HandwrittenDigitsDataset` class for loading images.
  - `Generator` and `Discriminator` neural network definitions.
  - Training loop with adversarial loss and Adam optimizers.
  - Image saving functionality using Matplotlib.

## Hyperparameters

- **Latent Dimension**: 100
- **Learning Rate**: 0.0002
- **Beta1 (Adam)**: 0.5
- **Batch Size**: 64
- **Epochs**: 50

Adjust these in the script if needed for your use case.

## Outputs

- **Generated Images**: Saved as `generated_digits_epoch_X.png` every 10 epochs, showing a 4x4 grid of generated digits.
- **Model Checkpoints**: `generator.pth` and `discriminator.pth` for resuming training or inference.

## Notes

- The code assumes a CUDA-enabled GPU is available. If not, it falls back to CPU, but training will be slower.
- Modify the dataset path in the script if your data is stored elsewhere.
- The GAN may require tuning (e.g., hyperparameters, network architecture) for optimal results on your specific dataset.
- Generated images may initially look noisy; quality typically improves with more epochs.

## ⚠️ Warning
This code is generated by an LLM (**Grok 3**) and may have certain problems. Please run this code at your own risk. This code is published for **educational purposes** only. The author is **not** responsible for any damage caused by running this code.
