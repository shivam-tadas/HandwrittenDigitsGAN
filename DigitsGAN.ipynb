{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fecc86fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aa9d76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class HandwrittenDigitsDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        for digit in range(10):\n",
    "            digit_dir = os.path.join(root_dir, str(digit), str(digit))\n",
    "            for img_name in os.listdir(digit_dir):\n",
    "                if img_name.endswith('.png'):\n",
    "                    self.image_paths.append(os.path.join(digit_dir, img_name))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('L')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab042dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator with batch normalization\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 1024),  # Larger initial layer\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2),  # Using LeakyReLU instead of ReLU\n",
    "            nn.Dropout(0.2),  # Less dropout\n",
    "            \n",
    "            nn.Linear(1024, 2048),  # Larger middle layer\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(2048, 28 * 28),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), 1, 28, 28)\n",
    "        return img\n",
    "\n",
    "# Discriminator\n",
    "# Remove sigmoid from discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 256),  # smaller network\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),  # add dropout\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),  # add dropout\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity\n",
    "\n",
    "# Gradient penalty function\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples, device):\n",
    "    alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=device)\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = torch.ones(real_samples.size(0), 1, device=device)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5812b044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "latent_dim = 100\n",
    "n_critic = 1  # Train D and G equally\n",
    "lambda_gp = 1  # Reduce gradient penalty weight\n",
    "lr_d = 0.000003  # Extremely low learning rate for discriminator\n",
    "lr_g = 0.000015   # Generator can learn 5x faster\n",
    "batch_size = 64\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 50\n",
    "\n",
    "# Dataset and DataLoader\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "dataset = HandwrittenDigitsDataset(root_dir=\"D:\\\\HandwrittenDigitsDataset\\\\dataset\", transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator(latent_dim).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Hyperparameters for WGAN\n",
    "n_critic = 2  # Train discriminator more often\n",
    "lambda_gp = 5  # Gradient penalty weight\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr_g, betas=(0.5, 0.9))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr_d, betas=(0.5, 0.9))\n",
    "scheduler_G = torch.optim.lr_scheduler.StepLR(optimizer_G, step_size=5, gamma=0.5)\n",
    "scheduler_D = torch.optim.lr_scheduler.StepLR(optimizer_D, step_size=5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e27f320b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shivam Tadas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:181.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/50] [Batch 0/1684] [D loss: 5.3101] [G loss: 0.1106]\n",
      "[Epoch 0/50] [Batch 100/1684] [D loss: 7.2247] [G loss: -0.8107]\n",
      "[Epoch 0/50] [Batch 200/1684] [D loss: 6.3433] [G loss: -0.6213]\n",
      "[Epoch 0/50] [Batch 300/1684] [D loss: 5.2045] [G loss: -0.2506]\n",
      "[Epoch 0/50] [Batch 400/1684] [D loss: 4.9039] [G loss: -0.1336]\n",
      "[Epoch 0/50] [Batch 500/1684] [D loss: 4.8631] [G loss: -0.1412]\n",
      "[Epoch 0/50] [Batch 600/1684] [D loss: 4.8231] [G loss: -0.1510]\n",
      "[Epoch 0/50] [Batch 700/1684] [D loss: 5.0398] [G loss: -0.2284]\n",
      "[Epoch 0/50] [Batch 800/1684] [D loss: 5.1428] [G loss: -0.2538]\n",
      "[Epoch 0/50] [Batch 900/1684] [D loss: 5.1062] [G loss: -0.2522]\n",
      "[Epoch 0/50] [Batch 1000/1684] [D loss: 4.9142] [G loss: -0.1666]\n",
      "[Epoch 0/50] [Batch 1100/1684] [D loss: 4.6879] [G loss: -0.1252]\n",
      "[Epoch 0/50] [Batch 1200/1684] [D loss: 4.7756] [G loss: -0.1387]\n",
      "[Epoch 0/50] [Batch 1300/1684] [D loss: 4.7602] [G loss: -0.1314]\n",
      "[Epoch 0/50] [Batch 1400/1684] [D loss: 4.6766] [G loss: -0.1452]\n",
      "[Epoch 0/50] [Batch 1500/1684] [D loss: 4.7395] [G loss: -0.1376]\n",
      "[Epoch 0/50] [Batch 1600/1684] [D loss: 4.7912] [G loss: -0.1202]\n",
      "[Epoch 1/50] [Batch 0/1684] [D loss: 4.9618] [G loss: -0.2125]\n",
      "[Epoch 1/50] [Batch 100/1684] [D loss: 5.0977] [G loss: -0.2577]\n",
      "[Epoch 1/50] [Batch 200/1684] [D loss: 4.9946] [G loss: -0.2142]\n",
      "[Epoch 1/50] [Batch 300/1684] [D loss: 5.0960] [G loss: -0.1775]\n",
      "[Epoch 1/50] [Batch 400/1684] [D loss: 4.8648] [G loss: -0.1460]\n",
      "[Epoch 1/50] [Batch 500/1684] [D loss: 4.8400] [G loss: -0.1104]\n",
      "[Epoch 1/50] [Batch 600/1684] [D loss: 4.8299] [G loss: -0.1326]\n",
      "[Epoch 1/50] [Batch 700/1684] [D loss: 4.9210] [G loss: -0.1691]\n",
      "[Epoch 1/50] [Batch 800/1684] [D loss: 4.8710] [G loss: -0.1320]\n",
      "[Epoch 1/50] [Batch 900/1684] [D loss: 4.9562] [G loss: -0.1758]\n",
      "[Epoch 1/50] [Batch 1000/1684] [D loss: 5.0852] [G loss: -0.1887]\n",
      "[Epoch 1/50] [Batch 1100/1684] [D loss: 5.0758] [G loss: -0.1475]\n",
      "[Epoch 1/50] [Batch 1200/1684] [D loss: 5.1133] [G loss: -0.1551]\n",
      "[Epoch 1/50] [Batch 1300/1684] [D loss: 5.0776] [G loss: -0.1591]\n",
      "[Epoch 1/50] [Batch 1400/1684] [D loss: 5.1107] [G loss: -0.1600]\n",
      "[Epoch 1/50] [Batch 1500/1684] [D loss: 5.3432] [G loss: -0.1567]\n",
      "[Epoch 1/50] [Batch 1600/1684] [D loss: 5.3778] [G loss: -0.1519]\n",
      "[Epoch 2/50] [Batch 0/1684] [D loss: 5.4325] [G loss: -0.1731]\n",
      "[Epoch 2/50] [Batch 100/1684] [D loss: 5.3459] [G loss: -0.1836]\n",
      "[Epoch 2/50] [Batch 200/1684] [D loss: 5.2282] [G loss: -0.1273]\n",
      "[Epoch 2/50] [Batch 300/1684] [D loss: 5.1322] [G loss: -0.1125]\n",
      "[Epoch 2/50] [Batch 400/1684] [D loss: 5.2466] [G loss: -0.1366]\n",
      "[Epoch 2/50] [Batch 500/1684] [D loss: 5.1480] [G loss: -0.1526]\n",
      "[Epoch 2/50] [Batch 600/1684] [D loss: 5.3634] [G loss: -0.1741]\n",
      "[Epoch 2/50] [Batch 700/1684] [D loss: 5.5365] [G loss: -0.2070]\n",
      "[Epoch 2/50] [Batch 800/1684] [D loss: 5.4095] [G loss: -0.1899]\n",
      "[Epoch 2/50] [Batch 900/1684] [D loss: 5.2858] [G loss: -0.1301]\n",
      "[Epoch 2/50] [Batch 1000/1684] [D loss: 5.3150] [G loss: -0.1364]\n",
      "[Epoch 2/50] [Batch 1100/1684] [D loss: 5.3128] [G loss: -0.1288]\n",
      "[Epoch 2/50] [Batch 1200/1684] [D loss: 5.2312] [G loss: -0.1526]\n",
      "[Epoch 2/50] [Batch 1300/1684] [D loss: 5.3895] [G loss: -0.1605]\n",
      "[Epoch 2/50] [Batch 1400/1684] [D loss: 5.4619] [G loss: -0.1844]\n",
      "[Epoch 2/50] [Batch 1500/1684] [D loss: 5.3250] [G loss: -0.1769]\n",
      "[Epoch 2/50] [Batch 1600/1684] [D loss: 5.2508] [G loss: -0.1271]\n",
      "[Epoch 3/50] [Batch 0/1684] [D loss: 5.0493] [G loss: -0.1073]\n",
      "[Epoch 3/50] [Batch 100/1684] [D loss: 4.9667] [G loss: -0.0787]\n",
      "[Epoch 3/50] [Batch 200/1684] [D loss: 5.0502] [G loss: -0.1135]\n",
      "[Epoch 3/50] [Batch 300/1684] [D loss: 5.2367] [G loss: -0.2223]\n",
      "[Epoch 3/50] [Batch 400/1684] [D loss: 5.3709] [G loss: -0.2491]\n",
      "[Epoch 3/50] [Batch 500/1684] [D loss: 5.3024] [G loss: -0.2228]\n",
      "[Epoch 3/50] [Batch 600/1684] [D loss: 5.4190] [G loss: -0.2147]\n",
      "[Epoch 3/50] [Batch 700/1684] [D loss: 5.3591] [G loss: -0.1894]\n",
      "[Epoch 3/50] [Batch 800/1684] [D loss: 5.4017] [G loss: -0.1875]\n",
      "[Epoch 3/50] [Batch 900/1684] [D loss: 5.5721] [G loss: -0.2556]\n",
      "[Epoch 3/50] [Batch 1000/1684] [D loss: 5.6843] [G loss: -0.2675]\n",
      "[Epoch 3/50] [Batch 1100/1684] [D loss: 5.6114] [G loss: -0.2163]\n",
      "[Epoch 3/50] [Batch 1200/1684] [D loss: 5.3616] [G loss: -0.1545]\n",
      "[Epoch 3/50] [Batch 1300/1684] [D loss: 5.1315] [G loss: -0.1269]\n",
      "[Epoch 3/50] [Batch 1400/1684] [D loss: 4.9948] [G loss: -0.0977]\n",
      "[Epoch 3/50] [Batch 1500/1684] [D loss: 5.0442] [G loss: -0.1837]\n",
      "[Epoch 3/50] [Batch 1600/1684] [D loss: 5.3152] [G loss: -0.2274]\n",
      "[Epoch 4/50] [Batch 0/1684] [D loss: 5.4518] [G loss: -0.2839]\n",
      "[Epoch 4/50] [Batch 100/1684] [D loss: 5.4899] [G loss: -0.2810]\n",
      "[Epoch 4/50] [Batch 200/1684] [D loss: 5.3727] [G loss: -0.2534]\n",
      "[Epoch 4/50] [Batch 300/1684] [D loss: 5.0071] [G loss: -0.1982]\n",
      "[Epoch 4/50] [Batch 400/1684] [D loss: 4.7578] [G loss: -0.1100]\n",
      "[Epoch 4/50] [Batch 500/1684] [D loss: 4.7696] [G loss: -0.1273]\n",
      "[Epoch 4/50] [Batch 600/1684] [D loss: 4.9095] [G loss: -0.1470]\n",
      "[Epoch 4/50] [Batch 700/1684] [D loss: 5.1239] [G loss: -0.2346]\n",
      "[Epoch 4/50] [Batch 800/1684] [D loss: 5.6263] [G loss: -0.2892]\n",
      "[Epoch 4/50] [Batch 900/1684] [D loss: 5.4577] [G loss: -0.2802]\n",
      "[Epoch 4/50] [Batch 1000/1684] [D loss: 5.3070] [G loss: -0.2812]\n",
      "[Epoch 4/50] [Batch 1100/1684] [D loss: 5.2650] [G loss: -0.2265]\n",
      "[Epoch 4/50] [Batch 1200/1684] [D loss: 5.3909] [G loss: -0.2948]\n",
      "[Epoch 4/50] [Batch 1300/1684] [D loss: 5.3989] [G loss: -0.2870]\n",
      "[Epoch 4/50] [Batch 1400/1684] [D loss: 5.5068] [G loss: -0.2901]\n",
      "[Epoch 4/50] [Batch 1500/1684] [D loss: 5.5471] [G loss: -0.3004]\n",
      "[Epoch 4/50] [Batch 1600/1684] [D loss: 5.3920] [G loss: -0.3013]\n",
      "[Epoch 5/50] [Batch 0/1684] [D loss: 5.4188] [G loss: -0.2750]\n",
      "[Epoch 5/50] [Batch 100/1684] [D loss: 5.3982] [G loss: -0.2727]\n",
      "[Epoch 5/50] [Batch 200/1684] [D loss: 5.3039] [G loss: -0.2451]\n",
      "[Epoch 5/50] [Batch 300/1684] [D loss: 5.3310] [G loss: -0.2753]\n",
      "[Epoch 5/50] [Batch 400/1684] [D loss: 5.3077] [G loss: -0.2229]\n",
      "[Epoch 5/50] [Batch 500/1684] [D loss: 5.1821] [G loss: -0.2305]\n",
      "[Epoch 5/50] [Batch 600/1684] [D loss: 5.2010] [G loss: -0.1818]\n",
      "[Epoch 5/50] [Batch 700/1684] [D loss: 4.9712] [G loss: -0.1745]\n",
      "[Epoch 5/50] [Batch 800/1684] [D loss: 4.6599] [G loss: -0.1138]\n",
      "[Epoch 5/50] [Batch 900/1684] [D loss: 4.5784] [G loss: -0.0638]\n",
      "[Epoch 5/50] [Batch 1000/1684] [D loss: 4.2559] [G loss: -0.0598]\n",
      "[Epoch 5/50] [Batch 1100/1684] [D loss: 4.2882] [G loss: -0.0628]\n",
      "[Epoch 5/50] [Batch 1200/1684] [D loss: 4.2950] [G loss: -0.1057]\n",
      "[Epoch 5/50] [Batch 1300/1684] [D loss: 4.4670] [G loss: -0.1607]\n",
      "[Epoch 5/50] [Batch 1400/1684] [D loss: 4.6632] [G loss: -0.2046]\n",
      "[Epoch 5/50] [Batch 1500/1684] [D loss: 4.6987] [G loss: -0.2448]\n",
      "[Epoch 5/50] [Batch 1600/1684] [D loss: 4.8439] [G loss: -0.2544]\n",
      "[Epoch 6/50] [Batch 0/1684] [D loss: 4.7926] [G loss: -0.2500]\n",
      "[Epoch 6/50] [Batch 100/1684] [D loss: 4.7190] [G loss: -0.2599]\n",
      "[Epoch 6/50] [Batch 200/1684] [D loss: 4.7705] [G loss: -0.2688]\n",
      "[Epoch 6/50] [Batch 300/1684] [D loss: 4.6918] [G loss: -0.2347]\n",
      "[Epoch 6/50] [Batch 400/1684] [D loss: 4.6562] [G loss: -0.2322]\n",
      "[Epoch 6/50] [Batch 500/1684] [D loss: 4.7715] [G loss: -0.2303]\n",
      "[Epoch 6/50] [Batch 600/1684] [D loss: 4.7821] [G loss: -0.2446]\n",
      "[Epoch 6/50] [Batch 700/1684] [D loss: 4.9185] [G loss: -0.2464]\n",
      "[Epoch 6/50] [Batch 800/1684] [D loss: 4.9822] [G loss: -0.3168]\n",
      "[Epoch 6/50] [Batch 900/1684] [D loss: 5.1570] [G loss: -0.3216]\n",
      "[Epoch 6/50] [Batch 1000/1684] [D loss: 5.2576] [G loss: -0.3497]\n",
      "[Epoch 6/50] [Batch 1100/1684] [D loss: 5.2179] [G loss: -0.3645]\n",
      "[Epoch 6/50] [Batch 1200/1684] [D loss: 5.2287] [G loss: -0.3623]\n",
      "[Epoch 6/50] [Batch 1300/1684] [D loss: 5.4452] [G loss: -0.3864]\n",
      "[Epoch 6/50] [Batch 1400/1684] [D loss: 5.5565] [G loss: -0.3925]\n",
      "[Epoch 6/50] [Batch 1500/1684] [D loss: 5.4840] [G loss: -0.3928]\n",
      "[Epoch 6/50] [Batch 1600/1684] [D loss: 5.6235] [G loss: -0.4127]\n",
      "[Epoch 7/50] [Batch 0/1684] [D loss: 5.4590] [G loss: -0.3593]\n",
      "[Epoch 7/50] [Batch 100/1684] [D loss: 5.1873] [G loss: -0.2969]\n",
      "[Epoch 7/50] [Batch 200/1684] [D loss: 4.9164] [G loss: -0.2338]\n",
      "[Epoch 7/50] [Batch 300/1684] [D loss: 4.6707] [G loss: -0.1969]\n",
      "[Epoch 7/50] [Batch 400/1684] [D loss: 4.5937] [G loss: -0.1699]\n",
      "[Epoch 7/50] [Batch 500/1684] [D loss: 4.6901] [G loss: -0.2076]\n",
      "[Epoch 7/50] [Batch 600/1684] [D loss: 4.7433] [G loss: -0.2333]\n",
      "[Epoch 7/50] [Batch 700/1684] [D loss: 4.9484] [G loss: -0.2956]\n",
      "[Epoch 7/50] [Batch 800/1684] [D loss: 5.2365] [G loss: -0.3593]\n",
      "[Epoch 7/50] [Batch 900/1684] [D loss: 5.2313] [G loss: -0.3662]\n",
      "[Epoch 7/50] [Batch 1000/1684] [D loss: 5.2940] [G loss: -0.3639]\n",
      "[Epoch 7/50] [Batch 1100/1684] [D loss: 5.3837] [G loss: -0.3753]\n",
      "[Epoch 7/50] [Batch 1200/1684] [D loss: 5.5757] [G loss: -0.3747]\n",
      "[Epoch 7/50] [Batch 1300/1684] [D loss: 5.4347] [G loss: -0.3594]\n",
      "[Epoch 7/50] [Batch 1400/1684] [D loss: 5.3814] [G loss: -0.3736]\n",
      "[Epoch 7/50] [Batch 1500/1684] [D loss: 5.3354] [G loss: -0.3533]\n",
      "[Epoch 7/50] [Batch 1600/1684] [D loss: 5.2372] [G loss: -0.2752]\n",
      "[Epoch 8/50] [Batch 0/1684] [D loss: 5.0051] [G loss: -0.2514]\n",
      "[Epoch 8/50] [Batch 100/1684] [D loss: 4.9395] [G loss: -0.1871]\n",
      "[Epoch 8/50] [Batch 200/1684] [D loss: 4.8029] [G loss: -0.1743]\n",
      "[Epoch 8/50] [Batch 300/1684] [D loss: 4.6972] [G loss: -0.1998]\n",
      "[Epoch 8/50] [Batch 400/1684] [D loss: 4.6594] [G loss: -0.1962]\n",
      "[Epoch 8/50] [Batch 500/1684] [D loss: 4.7324] [G loss: -0.2102]\n",
      "[Epoch 8/50] [Batch 600/1684] [D loss: 4.7767] [G loss: -0.2191]\n",
      "[Epoch 8/50] [Batch 700/1684] [D loss: 4.7831] [G loss: -0.2374]\n",
      "[Epoch 8/50] [Batch 800/1684] [D loss: 4.8393] [G loss: -0.2794]\n",
      "[Epoch 8/50] [Batch 900/1684] [D loss: 4.8404] [G loss: -0.3086]\n",
      "[Epoch 8/50] [Batch 1000/1684] [D loss: 4.9115] [G loss: -0.2922]\n",
      "[Epoch 8/50] [Batch 1100/1684] [D loss: 4.8034] [G loss: -0.2919]\n",
      "[Epoch 8/50] [Batch 1200/1684] [D loss: 4.7409] [G loss: -0.2573]\n",
      "[Epoch 8/50] [Batch 1300/1684] [D loss: 4.5863] [G loss: -0.2247]\n",
      "[Epoch 8/50] [Batch 1400/1684] [D loss: 4.5129] [G loss: -0.2318]\n",
      "[Epoch 8/50] [Batch 1500/1684] [D loss: 4.1342] [G loss: -0.1839]\n",
      "[Epoch 8/50] [Batch 1600/1684] [D loss: 4.1261] [G loss: -0.1357]\n",
      "[Epoch 9/50] [Batch 0/1684] [D loss: 3.8988] [G loss: -0.1964]\n",
      "[Epoch 9/50] [Batch 100/1684] [D loss: 3.7285] [G loss: 0.0133]\n",
      "[Epoch 9/50] [Batch 200/1684] [D loss: 4.4689] [G loss: -0.0902]\n",
      "[Epoch 9/50] [Batch 300/1684] [D loss: 4.0553] [G loss: -0.2317]\n",
      "[Epoch 9/50] [Batch 400/1684] [D loss: 3.5550] [G loss: -0.2087]\n",
      "[Epoch 9/50] [Batch 500/1684] [D loss: 3.9487] [G loss: -0.0880]\n",
      "[Epoch 9/50] [Batch 600/1684] [D loss: 5.4281] [G loss: -0.1595]\n",
      "[Epoch 9/50] [Batch 700/1684] [D loss: 4.0095] [G loss: 0.0596]\n",
      "[Epoch 9/50] [Batch 800/1684] [D loss: 4.4124] [G loss: -0.0392]\n",
      "[Epoch 9/50] [Batch 900/1684] [D loss: 3.5889] [G loss: -0.0622]\n",
      "[Epoch 9/50] [Batch 1000/1684] [D loss: 3.8564] [G loss: -0.0710]\n",
      "[Epoch 9/50] [Batch 1100/1684] [D loss: 3.9042] [G loss: -0.0720]\n",
      "[Epoch 9/50] [Batch 1200/1684] [D loss: 3.7226] [G loss: -0.0760]\n",
      "[Epoch 9/50] [Batch 1300/1684] [D loss: 3.1535] [G loss: 0.0665]\n",
      "[Epoch 9/50] [Batch 1400/1684] [D loss: 2.9328] [G loss: 0.0408]\n",
      "[Epoch 9/50] [Batch 1500/1684] [D loss: 2.8181] [G loss: 0.1696]\n",
      "[Epoch 9/50] [Batch 1600/1684] [D loss: 2.9601] [G loss: 0.2109]\n",
      "[Epoch 10/50] [Batch 0/1684] [D loss: 2.5788] [G loss: 0.3799]\n",
      "[Epoch 10/50] [Batch 100/1684] [D loss: 2.6166] [G loss: 0.2935]\n",
      "[Epoch 10/50] [Batch 200/1684] [D loss: 2.5374] [G loss: 0.2448]\n",
      "[Epoch 10/50] [Batch 300/1684] [D loss: 2.6969] [G loss: 0.4068]\n",
      "[Epoch 10/50] [Batch 400/1684] [D loss: 2.3363] [G loss: 0.3650]\n",
      "[Epoch 10/50] [Batch 500/1684] [D loss: 2.3088] [G loss: 0.3748]\n",
      "[Epoch 10/50] [Batch 600/1684] [D loss: 2.3119] [G loss: 0.4861]\n",
      "[Epoch 10/50] [Batch 700/1684] [D loss: 2.0747] [G loss: 0.4574]\n",
      "[Epoch 10/50] [Batch 800/1684] [D loss: 1.9820] [G loss: 0.5265]\n",
      "[Epoch 10/50] [Batch 900/1684] [D loss: 1.8156] [G loss: 0.4773]\n",
      "[Epoch 10/50] [Batch 1000/1684] [D loss: 1.5655] [G loss: 0.4474]\n",
      "[Epoch 10/50] [Batch 1100/1684] [D loss: 1.6659] [G loss: 0.5179]\n",
      "[Epoch 10/50] [Batch 1200/1684] [D loss: 1.7368] [G loss: 0.4690]\n",
      "[Epoch 10/50] [Batch 1300/1684] [D loss: 1.6341] [G loss: 0.5089]\n",
      "[Epoch 10/50] [Batch 1400/1684] [D loss: 1.5205] [G loss: 0.5448]\n",
      "[Epoch 10/50] [Batch 1500/1684] [D loss: 1.5500] [G loss: 0.5363]\n",
      "[Epoch 10/50] [Batch 1600/1684] [D loss: 1.5431] [G loss: 0.5361]\n",
      "[Epoch 11/50] [Batch 0/1684] [D loss: 1.5589] [G loss: 0.5302]\n",
      "[Epoch 11/50] [Batch 100/1684] [D loss: 1.3796] [G loss: 0.4962]\n",
      "[Epoch 11/50] [Batch 200/1684] [D loss: 1.1824] [G loss: 0.4777]\n",
      "[Epoch 11/50] [Batch 300/1684] [D loss: 1.1734] [G loss: 0.5172]\n",
      "[Epoch 11/50] [Batch 400/1684] [D loss: 1.1262] [G loss: 0.5770]\n",
      "[Epoch 11/50] [Batch 500/1684] [D loss: 1.1328] [G loss: 0.5302]\n",
      "[Epoch 11/50] [Batch 600/1684] [D loss: 0.9857] [G loss: 0.4377]\n",
      "[Epoch 11/50] [Batch 700/1684] [D loss: 1.0258] [G loss: 0.5249]\n",
      "[Epoch 11/50] [Batch 800/1684] [D loss: 1.0124] [G loss: 0.4811]\n",
      "[Epoch 11/50] [Batch 900/1684] [D loss: 0.9655] [G loss: 0.4814]\n",
      "[Epoch 11/50] [Batch 1000/1684] [D loss: 0.9190] [G loss: 0.4782]\n",
      "[Epoch 11/50] [Batch 1100/1684] [D loss: 1.0873] [G loss: 0.4312]\n",
      "[Epoch 11/50] [Batch 1200/1684] [D loss: 0.9569] [G loss: 0.4509]\n",
      "[Epoch 11/50] [Batch 1300/1684] [D loss: 1.0270] [G loss: 0.4732]\n",
      "[Epoch 11/50] [Batch 1400/1684] [D loss: 1.0089] [G loss: 0.4844]\n",
      "[Epoch 11/50] [Batch 1500/1684] [D loss: 0.9556] [G loss: 0.3679]\n",
      "[Epoch 11/50] [Batch 1600/1684] [D loss: 1.0694] [G loss: 0.4252]\n",
      "[Epoch 12/50] [Batch 0/1684] [D loss: 0.9837] [G loss: 0.3812]\n",
      "[Epoch 12/50] [Batch 100/1684] [D loss: 0.8807] [G loss: 0.4253]\n",
      "[Epoch 12/50] [Batch 200/1684] [D loss: 0.8635] [G loss: 0.4399]\n",
      "[Epoch 12/50] [Batch 300/1684] [D loss: 0.9153] [G loss: 0.4420]\n",
      "[Epoch 12/50] [Batch 400/1684] [D loss: 0.7027] [G loss: 0.5091]\n",
      "[Epoch 12/50] [Batch 500/1684] [D loss: 0.7178] [G loss: 0.4893]\n",
      "[Epoch 12/50] [Batch 600/1684] [D loss: 0.8860] [G loss: 0.5034]\n",
      "[Epoch 12/50] [Batch 700/1684] [D loss: 0.7367] [G loss: 0.4943]\n",
      "[Epoch 12/50] [Batch 800/1684] [D loss: 0.7293] [G loss: 0.5344]\n",
      "[Epoch 12/50] [Batch 900/1684] [D loss: 0.6522] [G loss: 0.4943]\n",
      "[Epoch 12/50] [Batch 1000/1684] [D loss: 0.6672] [G loss: 0.5297]\n",
      "[Epoch 12/50] [Batch 1100/1684] [D loss: 0.6723] [G loss: 0.4836]\n",
      "[Epoch 12/50] [Batch 1200/1684] [D loss: 0.7027] [G loss: 0.4815]\n",
      "[Epoch 12/50] [Batch 1300/1684] [D loss: 0.6995] [G loss: 0.4999]\n",
      "[Epoch 12/50] [Batch 1400/1684] [D loss: 0.6639] [G loss: 0.4576]\n",
      "[Epoch 12/50] [Batch 1500/1684] [D loss: 0.5737] [G loss: 0.5098]\n",
      "[Epoch 12/50] [Batch 1600/1684] [D loss: 0.7192] [G loss: 0.5116]\n",
      "[Epoch 13/50] [Batch 0/1684] [D loss: 0.6958] [G loss: 0.5420]\n",
      "[Epoch 13/50] [Batch 100/1684] [D loss: 0.6759] [G loss: 0.5919]\n",
      "[Epoch 13/50] [Batch 200/1684] [D loss: 0.7801] [G loss: 0.4879]\n",
      "[Epoch 13/50] [Batch 300/1684] [D loss: 0.6636] [G loss: 0.5552]\n",
      "[Epoch 13/50] [Batch 400/1684] [D loss: 0.6505] [G loss: 0.4997]\n",
      "[Epoch 13/50] [Batch 500/1684] [D loss: 0.5791] [G loss: 0.6417]\n",
      "[Epoch 13/50] [Batch 600/1684] [D loss: 0.6317] [G loss: 0.5294]\n",
      "[Epoch 13/50] [Batch 700/1684] [D loss: 0.6106] [G loss: 0.5602]\n",
      "[Epoch 13/50] [Batch 800/1684] [D loss: 0.5832] [G loss: 0.6025]\n",
      "[Epoch 13/50] [Batch 900/1684] [D loss: 0.5258] [G loss: 0.5939]\n",
      "[Epoch 13/50] [Batch 1000/1684] [D loss: 0.6748] [G loss: 0.5399]\n",
      "[Epoch 13/50] [Batch 1100/1684] [D loss: 0.5281] [G loss: 0.5043]\n",
      "[Epoch 13/50] [Batch 1200/1684] [D loss: 0.4990] [G loss: 0.5430]\n",
      "[Epoch 13/50] [Batch 1300/1684] [D loss: 0.7323] [G loss: 0.5380]\n",
      "[Epoch 13/50] [Batch 1400/1684] [D loss: 0.5767] [G loss: 0.5291]\n",
      "[Epoch 13/50] [Batch 1500/1684] [D loss: 0.5442] [G loss: 0.6130]\n",
      "[Epoch 13/50] [Batch 1600/1684] [D loss: 0.6004] [G loss: 0.5583]\n",
      "[Epoch 14/50] [Batch 0/1684] [D loss: 0.6506] [G loss: 0.6555]\n",
      "[Epoch 14/50] [Batch 100/1684] [D loss: 0.5758] [G loss: 0.6237]\n",
      "[Epoch 14/50] [Batch 200/1684] [D loss: 0.4647] [G loss: 0.6346]\n",
      "[Epoch 14/50] [Batch 300/1684] [D loss: 0.5578] [G loss: 0.5772]\n",
      "[Epoch 14/50] [Batch 400/1684] [D loss: 0.5996] [G loss: 0.6240]\n",
      "[Epoch 14/50] [Batch 500/1684] [D loss: 0.5269] [G loss: 0.4904]\n",
      "[Epoch 14/50] [Batch 600/1684] [D loss: 0.4794] [G loss: 0.5987]\n",
      "[Epoch 14/50] [Batch 700/1684] [D loss: 0.7159] [G loss: 0.6128]\n",
      "[Epoch 14/50] [Batch 800/1684] [D loss: 0.6551] [G loss: 0.4926]\n",
      "[Epoch 14/50] [Batch 900/1684] [D loss: 0.4895] [G loss: 0.5920]\n",
      "[Epoch 14/50] [Batch 1000/1684] [D loss: 0.6748] [G loss: 0.5807]\n",
      "[Epoch 14/50] [Batch 1100/1684] [D loss: 0.5264] [G loss: 0.5350]\n",
      "[Epoch 14/50] [Batch 1200/1684] [D loss: 0.5718] [G loss: 0.5435]\n",
      "[Epoch 14/50] [Batch 1300/1684] [D loss: 0.6201] [G loss: 0.4677]\n",
      "[Epoch 14/50] [Batch 1400/1684] [D loss: 0.5744] [G loss: 0.4849]\n",
      "[Epoch 14/50] [Batch 1500/1684] [D loss: 0.5845] [G loss: 0.5199]\n",
      "[Epoch 14/50] [Batch 1600/1684] [D loss: 0.5412] [G loss: 0.5111]\n",
      "[Epoch 15/50] [Batch 0/1684] [D loss: 0.6986] [G loss: 0.4376]\n",
      "[Epoch 15/50] [Batch 100/1684] [D loss: 0.5777] [G loss: 0.5012]\n",
      "[Epoch 15/50] [Batch 200/1684] [D loss: 0.6267] [G loss: 0.5916]\n",
      "[Epoch 15/50] [Batch 300/1684] [D loss: 0.5294] [G loss: 0.6200]\n",
      "[Epoch 15/50] [Batch 400/1684] [D loss: 0.6068] [G loss: 0.4829]\n",
      "[Epoch 15/50] [Batch 500/1684] [D loss: 0.6547] [G loss: 0.6872]\n",
      "[Epoch 15/50] [Batch 600/1684] [D loss: 0.5740] [G loss: 0.5444]\n",
      "[Epoch 15/50] [Batch 700/1684] [D loss: 0.5281] [G loss: 0.6113]\n",
      "[Epoch 15/50] [Batch 800/1684] [D loss: 0.5733] [G loss: 0.6724]\n",
      "[Epoch 15/50] [Batch 900/1684] [D loss: 0.5506] [G loss: 0.5858]\n",
      "[Epoch 15/50] [Batch 1000/1684] [D loss: 0.5217] [G loss: 0.5703]\n",
      "[Epoch 15/50] [Batch 1100/1684] [D loss: 0.5409] [G loss: 0.6027]\n",
      "[Epoch 15/50] [Batch 1200/1684] [D loss: 0.5035] [G loss: 0.5637]\n",
      "[Epoch 15/50] [Batch 1300/1684] [D loss: 0.5416] [G loss: 0.5785]\n",
      "[Epoch 15/50] [Batch 1400/1684] [D loss: 0.5219] [G loss: 0.6732]\n",
      "[Epoch 15/50] [Batch 1500/1684] [D loss: 0.4672] [G loss: 0.6747]\n",
      "[Epoch 15/50] [Batch 1600/1684] [D loss: 0.4737] [G loss: 0.6310]\n",
      "[Epoch 16/50] [Batch 0/1684] [D loss: 0.4418] [G loss: 0.5430]\n",
      "[Epoch 16/50] [Batch 100/1684] [D loss: 0.5678] [G loss: 0.5697]\n",
      "[Epoch 16/50] [Batch 200/1684] [D loss: 0.4553] [G loss: 0.5723]\n",
      "[Epoch 16/50] [Batch 300/1684] [D loss: 0.4676] [G loss: 0.5759]\n",
      "[Epoch 16/50] [Batch 400/1684] [D loss: 0.5185] [G loss: 0.6244]\n",
      "[Epoch 16/50] [Batch 500/1684] [D loss: 0.4762] [G loss: 0.5345]\n",
      "[Epoch 16/50] [Batch 600/1684] [D loss: 0.5182] [G loss: 0.5241]\n",
      "[Epoch 16/50] [Batch 700/1684] [D loss: 0.5444] [G loss: 0.5795]\n",
      "[Epoch 16/50] [Batch 800/1684] [D loss: 0.4729] [G loss: 0.5505]\n",
      "[Epoch 16/50] [Batch 900/1684] [D loss: 0.5115] [G loss: 0.5806]\n",
      "[Epoch 16/50] [Batch 1000/1684] [D loss: 0.5670] [G loss: 0.5717]\n",
      "[Epoch 16/50] [Batch 1100/1684] [D loss: 0.5094] [G loss: 0.5414]\n",
      "[Epoch 16/50] [Batch 1200/1684] [D loss: 0.5623] [G loss: 0.6093]\n",
      "[Epoch 16/50] [Batch 1300/1684] [D loss: 0.6027] [G loss: 0.6326]\n",
      "[Epoch 16/50] [Batch 1400/1684] [D loss: 0.4353] [G loss: 0.5349]\n",
      "[Epoch 16/50] [Batch 1500/1684] [D loss: 0.5173] [G loss: 0.6502]\n",
      "[Epoch 16/50] [Batch 1600/1684] [D loss: 0.5191] [G loss: 0.5933]\n",
      "[Epoch 17/50] [Batch 0/1684] [D loss: 0.6061] [G loss: 0.5543]\n",
      "[Epoch 17/50] [Batch 100/1684] [D loss: 0.4822] [G loss: 0.5968]\n",
      "[Epoch 17/50] [Batch 200/1684] [D loss: 0.5097] [G loss: 0.5660]\n",
      "[Epoch 17/50] [Batch 300/1684] [D loss: 0.3978] [G loss: 0.5536]\n",
      "[Epoch 17/50] [Batch 400/1684] [D loss: 0.4554] [G loss: 0.6426]\n",
      "[Epoch 17/50] [Batch 500/1684] [D loss: 0.4737] [G loss: 0.6500]\n",
      "[Epoch 17/50] [Batch 600/1684] [D loss: 0.4657] [G loss: 0.6853]\n",
      "[Epoch 17/50] [Batch 700/1684] [D loss: 0.4653] [G loss: 0.7005]\n",
      "[Epoch 17/50] [Batch 800/1684] [D loss: 0.4524] [G loss: 0.7213]\n",
      "[Epoch 17/50] [Batch 900/1684] [D loss: 0.4106] [G loss: 0.5988]\n",
      "[Epoch 17/50] [Batch 1000/1684] [D loss: 0.5071] [G loss: 0.6745]\n",
      "[Epoch 17/50] [Batch 1100/1684] [D loss: 0.5006] [G loss: 0.6174]\n",
      "[Epoch 17/50] [Batch 1200/1684] [D loss: 0.4433] [G loss: 0.6629]\n",
      "[Epoch 17/50] [Batch 1300/1684] [D loss: 0.4247] [G loss: 0.7594]\n",
      "[Epoch 17/50] [Batch 1400/1684] [D loss: 0.4465] [G loss: 0.6420]\n",
      "[Epoch 17/50] [Batch 1500/1684] [D loss: 0.5048] [G loss: 0.6514]\n",
      "[Epoch 17/50] [Batch 1600/1684] [D loss: 0.5485] [G loss: 0.6699]\n",
      "[Epoch 18/50] [Batch 0/1684] [D loss: 0.4383] [G loss: 0.6307]\n",
      "[Epoch 18/50] [Batch 100/1684] [D loss: 0.4030] [G loss: 0.6190]\n",
      "[Epoch 18/50] [Batch 200/1684] [D loss: 0.4610] [G loss: 0.6261]\n",
      "[Epoch 18/50] [Batch 300/1684] [D loss: 0.3818] [G loss: 0.6064]\n",
      "[Epoch 18/50] [Batch 400/1684] [D loss: 0.4948] [G loss: 0.5983]\n",
      "[Epoch 18/50] [Batch 500/1684] [D loss: 0.4181] [G loss: 0.6816]\n",
      "[Epoch 18/50] [Batch 600/1684] [D loss: 0.4219] [G loss: 0.6804]\n",
      "[Epoch 18/50] [Batch 700/1684] [D loss: 0.4514] [G loss: 0.7057]\n",
      "[Epoch 18/50] [Batch 800/1684] [D loss: 0.4325] [G loss: 0.7209]\n",
      "[Epoch 18/50] [Batch 900/1684] [D loss: 0.3929] [G loss: 0.6551]\n",
      "[Epoch 18/50] [Batch 1000/1684] [D loss: 0.3383] [G loss: 0.7215]\n",
      "[Epoch 18/50] [Batch 1100/1684] [D loss: 0.3980] [G loss: 0.6615]\n",
      "[Epoch 18/50] [Batch 1200/1684] [D loss: 0.3393] [G loss: 0.7181]\n",
      "[Epoch 18/50] [Batch 1300/1684] [D loss: 0.3496] [G loss: 0.6844]\n",
      "[Epoch 18/50] [Batch 1400/1684] [D loss: 0.2946] [G loss: 0.7175]\n",
      "[Epoch 18/50] [Batch 1500/1684] [D loss: 0.3982] [G loss: 0.8329]\n",
      "[Epoch 18/50] [Batch 1600/1684] [D loss: 0.3411] [G loss: 0.7208]\n",
      "[Epoch 19/50] [Batch 0/1684] [D loss: 0.3487] [G loss: 0.7934]\n",
      "[Epoch 19/50] [Batch 100/1684] [D loss: 0.3293] [G loss: 0.7562]\n",
      "[Epoch 19/50] [Batch 200/1684] [D loss: 0.3958] [G loss: 0.7006]\n",
      "[Epoch 19/50] [Batch 300/1684] [D loss: 0.3690] [G loss: 0.6878]\n",
      "[Epoch 19/50] [Batch 400/1684] [D loss: 0.3487] [G loss: 0.6848]\n",
      "[Epoch 19/50] [Batch 500/1684] [D loss: 0.3608] [G loss: 0.6665]\n",
      "[Epoch 19/50] [Batch 600/1684] [D loss: 0.3547] [G loss: 0.6836]\n",
      "[Epoch 19/50] [Batch 700/1684] [D loss: 0.3528] [G loss: 0.6693]\n",
      "[Epoch 19/50] [Batch 800/1684] [D loss: 0.4202] [G loss: 0.7163]\n",
      "[Epoch 19/50] [Batch 900/1684] [D loss: 0.3533] [G loss: 0.7109]\n",
      "[Epoch 19/50] [Batch 1000/1684] [D loss: 0.3645] [G loss: 0.7113]\n",
      "[Epoch 19/50] [Batch 1100/1684] [D loss: 0.4498] [G loss: 0.6216]\n",
      "[Epoch 19/50] [Batch 1200/1684] [D loss: 0.3337] [G loss: 0.6901]\n",
      "[Epoch 19/50] [Batch 1300/1684] [D loss: 0.2787] [G loss: 0.6816]\n",
      "[Epoch 19/50] [Batch 1400/1684] [D loss: 0.3992] [G loss: 0.6356]\n",
      "[Epoch 19/50] [Batch 1500/1684] [D loss: 0.3326] [G loss: 0.6052]\n",
      "[Epoch 19/50] [Batch 1600/1684] [D loss: 0.3636] [G loss: 0.6302]\n",
      "[Epoch 20/50] [Batch 0/1684] [D loss: 0.3113] [G loss: 0.6143]\n",
      "[Epoch 20/50] [Batch 100/1684] [D loss: 0.3537] [G loss: 0.6488]\n",
      "[Epoch 20/50] [Batch 200/1684] [D loss: 0.3596] [G loss: 0.6491]\n",
      "[Epoch 20/50] [Batch 300/1684] [D loss: 0.4167] [G loss: 0.5906]\n",
      "[Epoch 20/50] [Batch 400/1684] [D loss: 0.3890] [G loss: 0.6496]\n",
      "[Epoch 20/50] [Batch 500/1684] [D loss: 0.4519] [G loss: 0.6164]\n",
      "[Epoch 20/50] [Batch 600/1684] [D loss: 0.3860] [G loss: 0.5855]\n",
      "[Epoch 20/50] [Batch 700/1684] [D loss: 0.4220] [G loss: 0.5602]\n",
      "[Epoch 20/50] [Batch 800/1684] [D loss: 0.4506] [G loss: 0.5887]\n",
      "[Epoch 20/50] [Batch 900/1684] [D loss: 0.4749] [G loss: 0.6315]\n",
      "[Epoch 20/50] [Batch 1000/1684] [D loss: 0.4136] [G loss: 0.5932]\n",
      "[Epoch 20/50] [Batch 1100/1684] [D loss: 0.4527] [G loss: 0.6057]\n",
      "[Epoch 20/50] [Batch 1200/1684] [D loss: 0.4373] [G loss: 0.5574]\n",
      "[Epoch 20/50] [Batch 1300/1684] [D loss: 0.4216] [G loss: 0.5238]\n",
      "[Epoch 20/50] [Batch 1400/1684] [D loss: 0.4633] [G loss: 0.5523]\n",
      "[Epoch 20/50] [Batch 1500/1684] [D loss: 0.4146] [G loss: 0.5304]\n",
      "[Epoch 20/50] [Batch 1600/1684] [D loss: 0.4091] [G loss: 0.5620]\n",
      "[Epoch 21/50] [Batch 0/1684] [D loss: 0.5057] [G loss: 0.5497]\n",
      "[Epoch 21/50] [Batch 100/1684] [D loss: 0.4232] [G loss: 0.4514]\n",
      "[Epoch 21/50] [Batch 200/1684] [D loss: 0.4608] [G loss: 0.5949]\n",
      "[Epoch 21/50] [Batch 300/1684] [D loss: 0.3718] [G loss: 0.5762]\n",
      "[Epoch 21/50] [Batch 400/1684] [D loss: 0.4370] [G loss: 0.4805]\n",
      "[Epoch 21/50] [Batch 500/1684] [D loss: 0.4090] [G loss: 0.5142]\n",
      "[Epoch 21/50] [Batch 600/1684] [D loss: 0.5330] [G loss: 0.4679]\n",
      "[Epoch 21/50] [Batch 700/1684] [D loss: 0.3999] [G loss: 0.5498]\n",
      "[Epoch 21/50] [Batch 800/1684] [D loss: 0.4972] [G loss: 0.5290]\n",
      "[Epoch 21/50] [Batch 900/1684] [D loss: 0.4245] [G loss: 0.5775]\n",
      "[Epoch 21/50] [Batch 1000/1684] [D loss: 0.4206] [G loss: 0.5152]\n",
      "[Epoch 21/50] [Batch 1100/1684] [D loss: 0.4684] [G loss: 0.5523]\n",
      "[Epoch 21/50] [Batch 1200/1684] [D loss: 0.4047] [G loss: 0.4875]\n",
      "[Epoch 21/50] [Batch 1300/1684] [D loss: 0.5065] [G loss: 0.5357]\n",
      "[Epoch 21/50] [Batch 1400/1684] [D loss: 0.5076] [G loss: 0.5464]\n",
      "[Epoch 21/50] [Batch 1500/1684] [D loss: 0.5139] [G loss: 0.5333]\n",
      "[Epoch 21/50] [Batch 1600/1684] [D loss: 0.4147] [G loss: 0.5618]\n",
      "[Epoch 22/50] [Batch 0/1684] [D loss: 0.4376] [G loss: 0.5771]\n",
      "[Epoch 22/50] [Batch 100/1684] [D loss: 0.3450] [G loss: 0.6109]\n",
      "[Epoch 22/50] [Batch 200/1684] [D loss: 0.4530] [G loss: 0.5370]\n",
      "[Epoch 22/50] [Batch 300/1684] [D loss: 0.4314] [G loss: 0.5477]\n",
      "[Epoch 22/50] [Batch 400/1684] [D loss: 0.4679] [G loss: 0.5809]\n",
      "[Epoch 22/50] [Batch 500/1684] [D loss: 0.4541] [G loss: 0.5659]\n",
      "[Epoch 22/50] [Batch 600/1684] [D loss: 0.4296] [G loss: 0.5797]\n",
      "[Epoch 22/50] [Batch 700/1684] [D loss: 0.4866] [G loss: 0.5007]\n",
      "[Epoch 22/50] [Batch 800/1684] [D loss: 0.4398] [G loss: 0.6020]\n",
      "[Epoch 22/50] [Batch 900/1684] [D loss: 0.4356] [G loss: 0.5524]\n",
      "[Epoch 22/50] [Batch 1000/1684] [D loss: 0.4802] [G loss: 0.5827]\n",
      "[Epoch 22/50] [Batch 1100/1684] [D loss: 0.4886] [G loss: 0.5173]\n",
      "[Epoch 22/50] [Batch 1200/1684] [D loss: 0.5174] [G loss: 0.5329]\n",
      "[Epoch 22/50] [Batch 1300/1684] [D loss: 0.4844] [G loss: 0.5683]\n",
      "[Epoch 22/50] [Batch 1400/1684] [D loss: 0.4355] [G loss: 0.4987]\n",
      "[Epoch 22/50] [Batch 1500/1684] [D loss: 0.4855] [G loss: 0.5895]\n",
      "[Epoch 22/50] [Batch 1600/1684] [D loss: 0.5291] [G loss: 0.6081]\n",
      "[Epoch 23/50] [Batch 0/1684] [D loss: 0.4460] [G loss: 0.5828]\n",
      "[Epoch 23/50] [Batch 100/1684] [D loss: 0.4091] [G loss: 0.5335]\n",
      "[Epoch 23/50] [Batch 200/1684] [D loss: 0.4988] [G loss: 0.5856]\n",
      "[Epoch 23/50] [Batch 300/1684] [D loss: 0.4045] [G loss: 0.5306]\n",
      "[Epoch 23/50] [Batch 400/1684] [D loss: 0.5163] [G loss: 0.4770]\n",
      "[Epoch 23/50] [Batch 500/1684] [D loss: 0.4071] [G loss: 0.4984]\n",
      "[Epoch 23/50] [Batch 600/1684] [D loss: 0.5107] [G loss: 0.5395]\n",
      "[Epoch 23/50] [Batch 700/1684] [D loss: 0.4477] [G loss: 0.5455]\n",
      "[Epoch 23/50] [Batch 800/1684] [D loss: 0.4436] [G loss: 0.5405]\n",
      "[Epoch 23/50] [Batch 900/1684] [D loss: 0.4988] [G loss: 0.5630]\n",
      "[Epoch 23/50] [Batch 1000/1684] [D loss: 0.5123] [G loss: 0.5691]\n",
      "[Epoch 23/50] [Batch 1100/1684] [D loss: 0.4699] [G loss: 0.5109]\n",
      "[Epoch 23/50] [Batch 1200/1684] [D loss: 0.5277] [G loss: 0.5281]\n",
      "[Epoch 23/50] [Batch 1300/1684] [D loss: 0.5216] [G loss: 0.5516]\n",
      "[Epoch 23/50] [Batch 1400/1684] [D loss: 0.5005] [G loss: 0.5691]\n",
      "[Epoch 23/50] [Batch 1500/1684] [D loss: 0.4966] [G loss: 0.5214]\n",
      "[Epoch 23/50] [Batch 1600/1684] [D loss: 0.4777] [G loss: 0.5001]\n",
      "[Epoch 24/50] [Batch 0/1684] [D loss: 0.5646] [G loss: 0.5414]\n",
      "[Epoch 24/50] [Batch 100/1684] [D loss: 0.6149] [G loss: 0.5114]\n",
      "[Epoch 24/50] [Batch 200/1684] [D loss: 0.5251] [G loss: 0.5451]\n",
      "[Epoch 24/50] [Batch 300/1684] [D loss: 0.4244] [G loss: 0.5007]\n",
      "[Epoch 24/50] [Batch 400/1684] [D loss: 0.5388] [G loss: 0.5056]\n",
      "[Epoch 24/50] [Batch 500/1684] [D loss: 0.4971] [G loss: 0.4790]\n",
      "[Epoch 24/50] [Batch 600/1684] [D loss: 0.5289] [G loss: 0.4893]\n",
      "[Epoch 24/50] [Batch 700/1684] [D loss: 0.4783] [G loss: 0.4810]\n",
      "[Epoch 24/50] [Batch 800/1684] [D loss: 0.4474] [G loss: 0.4891]\n",
      "[Epoch 24/50] [Batch 900/1684] [D loss: 0.5538] [G loss: 0.4871]\n",
      "[Epoch 24/50] [Batch 1000/1684] [D loss: 0.4735] [G loss: 0.4687]\n",
      "[Epoch 24/50] [Batch 1100/1684] [D loss: 0.4962] [G loss: 0.5053]\n",
      "[Epoch 24/50] [Batch 1200/1684] [D loss: 0.4593] [G loss: 0.4466]\n",
      "[Epoch 24/50] [Batch 1300/1684] [D loss: 0.4520] [G loss: 0.5046]\n",
      "[Epoch 24/50] [Batch 1400/1684] [D loss: 0.4776] [G loss: 0.5321]\n",
      "[Epoch 24/50] [Batch 1500/1684] [D loss: 0.4642] [G loss: 0.5253]\n",
      "[Epoch 24/50] [Batch 1600/1684] [D loss: 0.4655] [G loss: 0.4953]\n",
      "[Epoch 25/50] [Batch 0/1684] [D loss: 0.5653] [G loss: 0.5067]\n",
      "[Epoch 25/50] [Batch 100/1684] [D loss: 0.5126] [G loss: 0.4740]\n",
      "[Epoch 25/50] [Batch 200/1684] [D loss: 0.4553] [G loss: 0.4194]\n",
      "[Epoch 25/50] [Batch 300/1684] [D loss: 0.5269] [G loss: 0.4585]\n",
      "[Epoch 25/50] [Batch 400/1684] [D loss: 0.4937] [G loss: 0.4642]\n",
      "[Epoch 25/50] [Batch 500/1684] [D loss: 0.5251] [G loss: 0.4367]\n",
      "[Epoch 25/50] [Batch 600/1684] [D loss: 0.5336] [G loss: 0.4556]\n",
      "[Epoch 25/50] [Batch 700/1684] [D loss: 0.5620] [G loss: 0.4559]\n",
      "[Epoch 25/50] [Batch 800/1684] [D loss: 0.5320] [G loss: 0.4809]\n",
      "[Epoch 25/50] [Batch 900/1684] [D loss: 0.5509] [G loss: 0.4746]\n",
      "[Epoch 25/50] [Batch 1000/1684] [D loss: 0.4899] [G loss: 0.4051]\n",
      "[Epoch 25/50] [Batch 1100/1684] [D loss: 0.5201] [G loss: 0.4512]\n",
      "[Epoch 25/50] [Batch 1200/1684] [D loss: 0.5247] [G loss: 0.4525]\n",
      "[Epoch 25/50] [Batch 1300/1684] [D loss: 0.5928] [G loss: 0.4465]\n",
      "[Epoch 25/50] [Batch 1400/1684] [D loss: 0.5332] [G loss: 0.4414]\n",
      "[Epoch 25/50] [Batch 1500/1684] [D loss: 0.5698] [G loss: 0.4699]\n",
      "[Epoch 25/50] [Batch 1600/1684] [D loss: 0.5966] [G loss: 0.4537]\n",
      "[Epoch 26/50] [Batch 0/1684] [D loss: 0.5143] [G loss: 0.4484]\n",
      "[Epoch 26/50] [Batch 100/1684] [D loss: 0.4484] [G loss: 0.4563]\n",
      "[Epoch 26/50] [Batch 200/1684] [D loss: 0.5121] [G loss: 0.4677]\n",
      "[Epoch 26/50] [Batch 300/1684] [D loss: 0.4860] [G loss: 0.4684]\n",
      "[Epoch 26/50] [Batch 400/1684] [D loss: 0.4964] [G loss: 0.4619]\n",
      "[Epoch 26/50] [Batch 500/1684] [D loss: 0.5338] [G loss: 0.4569]\n",
      "[Epoch 26/50] [Batch 600/1684] [D loss: 0.4788] [G loss: 0.4895]\n",
      "[Epoch 26/50] [Batch 700/1684] [D loss: 0.5111] [G loss: 0.4341]\n",
      "[Epoch 26/50] [Batch 800/1684] [D loss: 0.4838] [G loss: 0.4923]\n",
      "[Epoch 26/50] [Batch 900/1684] [D loss: 0.4843] [G loss: 0.4631]\n",
      "[Epoch 26/50] [Batch 1000/1684] [D loss: 0.4356] [G loss: 0.4692]\n",
      "[Epoch 26/50] [Batch 1100/1684] [D loss: 0.5036] [G loss: 0.5080]\n",
      "[Epoch 26/50] [Batch 1200/1684] [D loss: 0.4804] [G loss: 0.4862]\n",
      "[Epoch 26/50] [Batch 1300/1684] [D loss: 0.4686] [G loss: 0.4454]\n",
      "[Epoch 26/50] [Batch 1400/1684] [D loss: 0.5009] [G loss: 0.5172]\n",
      "[Epoch 26/50] [Batch 1500/1684] [D loss: 0.4666] [G loss: 0.5222]\n",
      "[Epoch 26/50] [Batch 1600/1684] [D loss: 0.4714] [G loss: 0.4963]\n",
      "[Epoch 27/50] [Batch 0/1684] [D loss: 0.4977] [G loss: 0.4676]\n",
      "[Epoch 27/50] [Batch 100/1684] [D loss: 0.5148] [G loss: 0.5278]\n",
      "[Epoch 27/50] [Batch 200/1684] [D loss: 0.4670] [G loss: 0.5462]\n",
      "[Epoch 27/50] [Batch 300/1684] [D loss: 0.4876] [G loss: 0.5096]\n",
      "[Epoch 27/50] [Batch 400/1684] [D loss: 0.4671] [G loss: 0.4775]\n",
      "[Epoch 27/50] [Batch 500/1684] [D loss: 0.4669] [G loss: 0.5050]\n",
      "[Epoch 27/50] [Batch 600/1684] [D loss: 0.5033] [G loss: 0.5065]\n",
      "[Epoch 27/50] [Batch 700/1684] [D loss: 0.5011] [G loss: 0.4890]\n",
      "[Epoch 27/50] [Batch 800/1684] [D loss: 0.4520] [G loss: 0.4973]\n",
      "[Epoch 27/50] [Batch 900/1684] [D loss: 0.5450] [G loss: 0.5043]\n",
      "[Epoch 27/50] [Batch 1000/1684] [D loss: 0.4940] [G loss: 0.5113]\n",
      "[Epoch 27/50] [Batch 1100/1684] [D loss: 0.4567] [G loss: 0.5563]\n",
      "[Epoch 27/50] [Batch 1200/1684] [D loss: 0.5342] [G loss: 0.4923]\n",
      "[Epoch 27/50] [Batch 1300/1684] [D loss: 0.4613] [G loss: 0.5248]\n",
      "[Epoch 27/50] [Batch 1400/1684] [D loss: 0.4560] [G loss: 0.4793]\n",
      "[Epoch 27/50] [Batch 1500/1684] [D loss: 0.4643] [G loss: 0.4331]\n",
      "[Epoch 27/50] [Batch 1600/1684] [D loss: 0.5015] [G loss: 0.4682]\n",
      "[Epoch 28/50] [Batch 0/1684] [D loss: 0.5171] [G loss: 0.5224]\n",
      "[Epoch 28/50] [Batch 100/1684] [D loss: 0.3995] [G loss: 0.4637]\n",
      "[Epoch 28/50] [Batch 200/1684] [D loss: 0.4943] [G loss: 0.5508]\n",
      "[Epoch 28/50] [Batch 300/1684] [D loss: 0.4666] [G loss: 0.5188]\n",
      "[Epoch 28/50] [Batch 400/1684] [D loss: 0.4785] [G loss: 0.5069]\n",
      "[Epoch 28/50] [Batch 500/1684] [D loss: 0.4560] [G loss: 0.4826]\n",
      "[Epoch 28/50] [Batch 600/1684] [D loss: 0.4271] [G loss: 0.5471]\n",
      "[Epoch 28/50] [Batch 700/1684] [D loss: 0.4202] [G loss: 0.4858]\n",
      "[Epoch 28/50] [Batch 800/1684] [D loss: 0.4102] [G loss: 0.5225]\n",
      "[Epoch 28/50] [Batch 900/1684] [D loss: 0.4984] [G loss: 0.5047]\n",
      "[Epoch 28/50] [Batch 1000/1684] [D loss: 0.5302] [G loss: 0.5346]\n",
      "[Epoch 28/50] [Batch 1100/1684] [D loss: 0.4932] [G loss: 0.4504]\n",
      "[Epoch 28/50] [Batch 1200/1684] [D loss: 0.4630] [G loss: 0.5488]\n",
      "[Epoch 28/50] [Batch 1300/1684] [D loss: 0.4204] [G loss: 0.5230]\n",
      "[Epoch 28/50] [Batch 1400/1684] [D loss: 0.4444] [G loss: 0.5187]\n",
      "[Epoch 28/50] [Batch 1500/1684] [D loss: 0.4769] [G loss: 0.5107]\n",
      "[Epoch 28/50] [Batch 1600/1684] [D loss: 0.3912] [G loss: 0.5044]\n",
      "[Epoch 29/50] [Batch 0/1684] [D loss: 0.5575] [G loss: 0.4986]\n",
      "[Epoch 29/50] [Batch 100/1684] [D loss: 0.4790] [G loss: 0.4851]\n",
      "[Epoch 29/50] [Batch 200/1684] [D loss: 0.5127] [G loss: 0.5240]\n",
      "[Epoch 29/50] [Batch 300/1684] [D loss: 0.3934] [G loss: 0.5448]\n",
      "[Epoch 29/50] [Batch 400/1684] [D loss: 0.4401] [G loss: 0.5248]\n",
      "[Epoch 29/50] [Batch 500/1684] [D loss: 0.3750] [G loss: 0.5375]\n",
      "[Epoch 29/50] [Batch 600/1684] [D loss: 0.4268] [G loss: 0.5263]\n",
      "[Epoch 29/50] [Batch 700/1684] [D loss: 0.4291] [G loss: 0.5084]\n",
      "[Epoch 29/50] [Batch 800/1684] [D loss: 0.4783] [G loss: 0.4922]\n",
      "[Epoch 29/50] [Batch 900/1684] [D loss: 0.4868] [G loss: 0.5169]\n",
      "[Epoch 29/50] [Batch 1000/1684] [D loss: 0.4293] [G loss: 0.5081]\n",
      "[Epoch 29/50] [Batch 1100/1684] [D loss: 0.4627] [G loss: 0.4369]\n",
      "[Epoch 29/50] [Batch 1200/1684] [D loss: 0.4411] [G loss: 0.5085]\n",
      "[Epoch 29/50] [Batch 1300/1684] [D loss: 0.4796] [G loss: 0.4978]\n",
      "[Epoch 29/50] [Batch 1400/1684] [D loss: 0.4277] [G loss: 0.4575]\n",
      "[Epoch 29/50] [Batch 1500/1684] [D loss: 0.4981] [G loss: 0.4873]\n",
      "[Epoch 29/50] [Batch 1600/1684] [D loss: 0.4159] [G loss: 0.5027]\n",
      "[Epoch 30/50] [Batch 0/1684] [D loss: 0.4296] [G loss: 0.5114]\n",
      "[Epoch 30/50] [Batch 100/1684] [D loss: 0.4873] [G loss: 0.4714]\n",
      "[Epoch 30/50] [Batch 200/1684] [D loss: 0.4078] [G loss: 0.5688]\n",
      "[Epoch 30/50] [Batch 300/1684] [D loss: 0.4559] [G loss: 0.5555]\n",
      "[Epoch 30/50] [Batch 400/1684] [D loss: 0.4805] [G loss: 0.5454]\n",
      "[Epoch 30/50] [Batch 500/1684] [D loss: 0.4015] [G loss: 0.5529]\n",
      "[Epoch 30/50] [Batch 600/1684] [D loss: 0.4824] [G loss: 0.5552]\n",
      "[Epoch 30/50] [Batch 700/1684] [D loss: 0.4300] [G loss: 0.5227]\n",
      "[Epoch 30/50] [Batch 800/1684] [D loss: 0.4328] [G loss: 0.5055]\n",
      "[Epoch 30/50] [Batch 900/1684] [D loss: 0.4054] [G loss: 0.4902]\n",
      "[Epoch 30/50] [Batch 1000/1684] [D loss: 0.4599] [G loss: 0.5325]\n",
      "[Epoch 30/50] [Batch 1100/1684] [D loss: 0.5039] [G loss: 0.4913]\n",
      "[Epoch 30/50] [Batch 1200/1684] [D loss: 0.5159] [G loss: 0.5422]\n",
      "[Epoch 30/50] [Batch 1300/1684] [D loss: 0.3713] [G loss: 0.4908]\n",
      "[Epoch 30/50] [Batch 1400/1684] [D loss: 0.5020] [G loss: 0.5164]\n",
      "[Epoch 30/50] [Batch 1500/1684] [D loss: 0.4062] [G loss: 0.5518]\n",
      "[Epoch 30/50] [Batch 1600/1684] [D loss: 0.4221] [G loss: 0.5055]\n",
      "[Epoch 31/50] [Batch 0/1684] [D loss: 0.4636] [G loss: 0.4928]\n",
      "[Epoch 31/50] [Batch 100/1684] [D loss: 0.4192] [G loss: 0.5335]\n",
      "[Epoch 31/50] [Batch 200/1684] [D loss: 0.4752] [G loss: 0.5507]\n",
      "[Epoch 31/50] [Batch 300/1684] [D loss: 0.4082] [G loss: 0.5255]\n",
      "[Epoch 31/50] [Batch 400/1684] [D loss: 0.4962] [G loss: 0.5178]\n",
      "[Epoch 31/50] [Batch 500/1684] [D loss: 0.4352] [G loss: 0.5194]\n",
      "[Epoch 31/50] [Batch 600/1684] [D loss: 0.5198] [G loss: 0.5379]\n",
      "[Epoch 31/50] [Batch 700/1684] [D loss: 0.4635] [G loss: 0.4972]\n",
      "[Epoch 31/50] [Batch 800/1684] [D loss: 0.4239] [G loss: 0.5088]\n",
      "[Epoch 31/50] [Batch 900/1684] [D loss: 0.4052] [G loss: 0.5628]\n",
      "[Epoch 31/50] [Batch 1000/1684] [D loss: 0.3964] [G loss: 0.5397]\n",
      "[Epoch 31/50] [Batch 1100/1684] [D loss: 0.3975] [G loss: 0.5070]\n",
      "[Epoch 31/50] [Batch 1200/1684] [D loss: 0.3742] [G loss: 0.5342]\n",
      "[Epoch 31/50] [Batch 1300/1684] [D loss: 0.4950] [G loss: 0.5690]\n",
      "[Epoch 31/50] [Batch 1400/1684] [D loss: 0.4085] [G loss: 0.5290]\n",
      "[Epoch 31/50] [Batch 1500/1684] [D loss: 0.4277] [G loss: 0.4494]\n",
      "[Epoch 31/50] [Batch 1600/1684] [D loss: 0.4913] [G loss: 0.5289]\n",
      "[Epoch 32/50] [Batch 0/1684] [D loss: 0.4355] [G loss: 0.4889]\n",
      "[Epoch 32/50] [Batch 100/1684] [D loss: 0.4133] [G loss: 0.5213]\n",
      "[Epoch 32/50] [Batch 200/1684] [D loss: 0.4255] [G loss: 0.5664]\n",
      "[Epoch 32/50] [Batch 300/1684] [D loss: 0.4556] [G loss: 0.5359]\n",
      "[Epoch 32/50] [Batch 400/1684] [D loss: 0.4552] [G loss: 0.5278]\n",
      "[Epoch 32/50] [Batch 500/1684] [D loss: 0.3950] [G loss: 0.4509]\n",
      "[Epoch 32/50] [Batch 600/1684] [D loss: 0.4307] [G loss: 0.5177]\n",
      "[Epoch 32/50] [Batch 700/1684] [D loss: 0.4258] [G loss: 0.5528]\n",
      "[Epoch 32/50] [Batch 800/1684] [D loss: 0.4549] [G loss: 0.5267]\n",
      "[Epoch 32/50] [Batch 900/1684] [D loss: 0.4655] [G loss: 0.4514]\n",
      "[Epoch 32/50] [Batch 1000/1684] [D loss: 0.4557] [G loss: 0.5534]\n",
      "[Epoch 32/50] [Batch 1100/1684] [D loss: 0.5059] [G loss: 0.5064]\n",
      "[Epoch 32/50] [Batch 1200/1684] [D loss: 0.4354] [G loss: 0.5333]\n",
      "[Epoch 32/50] [Batch 1300/1684] [D loss: 0.4378] [G loss: 0.5626]\n",
      "[Epoch 32/50] [Batch 1400/1684] [D loss: 0.4244] [G loss: 0.5065]\n",
      "[Epoch 32/50] [Batch 1500/1684] [D loss: 0.4291] [G loss: 0.5046]\n",
      "[Epoch 32/50] [Batch 1600/1684] [D loss: 0.3963] [G loss: 0.5136]\n",
      "[Epoch 33/50] [Batch 0/1684] [D loss: 0.5097] [G loss: 0.5410]\n",
      "[Epoch 33/50] [Batch 100/1684] [D loss: 0.4417] [G loss: 0.5394]\n",
      "[Epoch 33/50] [Batch 200/1684] [D loss: 0.4279] [G loss: 0.5002]\n",
      "[Epoch 33/50] [Batch 300/1684] [D loss: 0.3567] [G loss: 0.5364]\n",
      "[Epoch 33/50] [Batch 400/1684] [D loss: 0.4342] [G loss: 0.5060]\n",
      "[Epoch 33/50] [Batch 500/1684] [D loss: 0.4667] [G loss: 0.5583]\n",
      "[Epoch 33/50] [Batch 600/1684] [D loss: 0.4138] [G loss: 0.6108]\n",
      "[Epoch 33/50] [Batch 700/1684] [D loss: 0.4448] [G loss: 0.5406]\n",
      "[Epoch 33/50] [Batch 800/1684] [D loss: 0.4397] [G loss: 0.5041]\n",
      "[Epoch 33/50] [Batch 900/1684] [D loss: 0.5036] [G loss: 0.5764]\n",
      "[Epoch 33/50] [Batch 1000/1684] [D loss: 0.5077] [G loss: 0.5423]\n",
      "[Epoch 33/50] [Batch 1100/1684] [D loss: 0.4358] [G loss: 0.5564]\n",
      "[Epoch 33/50] [Batch 1200/1684] [D loss: 0.4379] [G loss: 0.6381]\n",
      "[Epoch 33/50] [Batch 1300/1684] [D loss: 0.4937] [G loss: 0.5900]\n",
      "[Epoch 33/50] [Batch 1400/1684] [D loss: 0.3912] [G loss: 0.5548]\n",
      "[Epoch 33/50] [Batch 1500/1684] [D loss: 0.4546] [G loss: 0.5705]\n",
      "[Epoch 33/50] [Batch 1600/1684] [D loss: 0.4203] [G loss: 0.5423]\n",
      "[Epoch 34/50] [Batch 0/1684] [D loss: 0.4581] [G loss: 0.5801]\n",
      "[Epoch 34/50] [Batch 100/1684] [D loss: 0.4633] [G loss: 0.4835]\n",
      "[Epoch 34/50] [Batch 200/1684] [D loss: 0.4130] [G loss: 0.5624]\n",
      "[Epoch 34/50] [Batch 300/1684] [D loss: 0.4074] [G loss: 0.5350]\n",
      "[Epoch 34/50] [Batch 400/1684] [D loss: 0.4295] [G loss: 0.5115]\n",
      "[Epoch 34/50] [Batch 500/1684] [D loss: 0.4014] [G loss: 0.6029]\n",
      "[Epoch 34/50] [Batch 600/1684] [D loss: 0.4106] [G loss: 0.6136]\n",
      "[Epoch 34/50] [Batch 700/1684] [D loss: 0.4370] [G loss: 0.5804]\n",
      "[Epoch 34/50] [Batch 800/1684] [D loss: 0.3697] [G loss: 0.5371]\n",
      "[Epoch 34/50] [Batch 900/1684] [D loss: 0.4640] [G loss: 0.5323]\n",
      "[Epoch 34/50] [Batch 1000/1684] [D loss: 0.5086] [G loss: 0.5925]\n",
      "[Epoch 34/50] [Batch 1100/1684] [D loss: 0.4790] [G loss: 0.5347]\n",
      "[Epoch 34/50] [Batch 1200/1684] [D loss: 0.4438] [G loss: 0.5778]\n",
      "[Epoch 34/50] [Batch 1300/1684] [D loss: 0.4000] [G loss: 0.5192]\n",
      "[Epoch 34/50] [Batch 1400/1684] [D loss: 0.4945] [G loss: 0.5613]\n",
      "[Epoch 34/50] [Batch 1500/1684] [D loss: 0.5033] [G loss: 0.5757]\n",
      "[Epoch 34/50] [Batch 1600/1684] [D loss: 0.4014] [G loss: 0.5290]\n",
      "[Epoch 35/50] [Batch 0/1684] [D loss: 0.4151] [G loss: 0.6094]\n",
      "[Epoch 35/50] [Batch 100/1684] [D loss: 0.3737] [G loss: 0.5801]\n",
      "[Epoch 35/50] [Batch 200/1684] [D loss: 0.5081] [G loss: 0.5112]\n",
      "[Epoch 35/50] [Batch 300/1684] [D loss: 0.3707] [G loss: 0.4593]\n",
      "[Epoch 35/50] [Batch 400/1684] [D loss: 0.4108] [G loss: 0.5732]\n",
      "[Epoch 35/50] [Batch 500/1684] [D loss: 0.4343] [G loss: 0.5830]\n",
      "[Epoch 35/50] [Batch 600/1684] [D loss: 0.4025] [G loss: 0.5712]\n",
      "[Epoch 35/50] [Batch 700/1684] [D loss: 0.4848] [G loss: 0.5551]\n",
      "[Epoch 35/50] [Batch 800/1684] [D loss: 0.3767] [G loss: 0.5547]\n",
      "[Epoch 35/50] [Batch 900/1684] [D loss: 0.3812] [G loss: 0.5803]\n",
      "[Epoch 35/50] [Batch 1000/1684] [D loss: 0.4369] [G loss: 0.5957]\n",
      "[Epoch 35/50] [Batch 1100/1684] [D loss: 0.4046] [G loss: 0.5923]\n",
      "[Epoch 35/50] [Batch 1200/1684] [D loss: 0.4024] [G loss: 0.5546]\n",
      "[Epoch 35/50] [Batch 1300/1684] [D loss: 0.3921] [G loss: 0.5788]\n",
      "[Epoch 35/50] [Batch 1400/1684] [D loss: 0.4204] [G loss: 0.5857]\n",
      "[Epoch 35/50] [Batch 1500/1684] [D loss: 0.3991] [G loss: 0.6239]\n",
      "[Epoch 35/50] [Batch 1600/1684] [D loss: 0.3914] [G loss: 0.5570]\n",
      "[Epoch 36/50] [Batch 0/1684] [D loss: 0.4589] [G loss: 0.5785]\n",
      "[Epoch 36/50] [Batch 100/1684] [D loss: 0.4903] [G loss: 0.5922]\n",
      "[Epoch 36/50] [Batch 200/1684] [D loss: 0.4470] [G loss: 0.5967]\n",
      "[Epoch 36/50] [Batch 300/1684] [D loss: 0.4573] [G loss: 0.5745]\n",
      "[Epoch 36/50] [Batch 400/1684] [D loss: 0.4686] [G loss: 0.5567]\n",
      "[Epoch 36/50] [Batch 500/1684] [D loss: 0.4300] [G loss: 0.4979]\n",
      "[Epoch 36/50] [Batch 600/1684] [D loss: 0.4790] [G loss: 0.6482]\n",
      "[Epoch 36/50] [Batch 700/1684] [D loss: 0.3879] [G loss: 0.5059]\n",
      "[Epoch 36/50] [Batch 800/1684] [D loss: 0.5140] [G loss: 0.5838]\n",
      "[Epoch 36/50] [Batch 900/1684] [D loss: 0.3787] [G loss: 0.5683]\n",
      "[Epoch 36/50] [Batch 1000/1684] [D loss: 0.3962] [G loss: 0.5806]\n",
      "[Epoch 36/50] [Batch 1100/1684] [D loss: 0.4755] [G loss: 0.4936]\n",
      "[Epoch 36/50] [Batch 1200/1684] [D loss: 0.4146] [G loss: 0.5803]\n",
      "[Epoch 36/50] [Batch 1300/1684] [D loss: 0.4427] [G loss: 0.5282]\n",
      "[Epoch 36/50] [Batch 1400/1684] [D loss: 0.4073] [G loss: 0.5439]\n",
      "[Epoch 36/50] [Batch 1500/1684] [D loss: 0.4882] [G loss: 0.5632]\n",
      "[Epoch 36/50] [Batch 1600/1684] [D loss: 0.4551] [G loss: 0.5141]\n",
      "[Epoch 37/50] [Batch 0/1684] [D loss: 0.4180] [G loss: 0.6140]\n",
      "[Epoch 37/50] [Batch 100/1684] [D loss: 0.4048] [G loss: 0.4906]\n",
      "[Epoch 37/50] [Batch 200/1684] [D loss: 0.4872] [G loss: 0.5646]\n",
      "[Epoch 37/50] [Batch 300/1684] [D loss: 0.4724] [G loss: 0.5215]\n",
      "[Epoch 37/50] [Batch 400/1684] [D loss: 0.4776] [G loss: 0.5315]\n",
      "[Epoch 37/50] [Batch 500/1684] [D loss: 0.4468] [G loss: 0.4987]\n",
      "[Epoch 37/50] [Batch 600/1684] [D loss: 0.5188] [G loss: 0.5217]\n",
      "[Epoch 37/50] [Batch 700/1684] [D loss: 0.4790] [G loss: 0.5776]\n",
      "[Epoch 37/50] [Batch 800/1684] [D loss: 0.4733] [G loss: 0.5492]\n",
      "[Epoch 37/50] [Batch 900/1684] [D loss: 0.4256] [G loss: 0.5341]\n",
      "[Epoch 37/50] [Batch 1000/1684] [D loss: 0.4710] [G loss: 0.5850]\n",
      "[Epoch 37/50] [Batch 1100/1684] [D loss: 0.4388] [G loss: 0.5145]\n",
      "[Epoch 37/50] [Batch 1200/1684] [D loss: 0.5368] [G loss: 0.5270]\n",
      "[Epoch 37/50] [Batch 1300/1684] [D loss: 0.4751] [G loss: 0.5875]\n",
      "[Epoch 37/50] [Batch 1400/1684] [D loss: 0.4706] [G loss: 0.5996]\n",
      "[Epoch 37/50] [Batch 1500/1684] [D loss: 0.4339] [G loss: 0.5149]\n",
      "[Epoch 37/50] [Batch 1600/1684] [D loss: 0.3977] [G loss: 0.4702]\n",
      "[Epoch 38/50] [Batch 0/1684] [D loss: 0.4086] [G loss: 0.5408]\n",
      "[Epoch 38/50] [Batch 100/1684] [D loss: 0.5065] [G loss: 0.5152]\n",
      "[Epoch 38/50] [Batch 200/1684] [D loss: 0.4196] [G loss: 0.5231]\n",
      "[Epoch 38/50] [Batch 300/1684] [D loss: 0.3946] [G loss: 0.5420]\n",
      "[Epoch 38/50] [Batch 400/1684] [D loss: 0.3814] [G loss: 0.5530]\n",
      "[Epoch 38/50] [Batch 500/1684] [D loss: 0.4705] [G loss: 0.4877]\n",
      "[Epoch 38/50] [Batch 600/1684] [D loss: 0.4124] [G loss: 0.4914]\n",
      "[Epoch 38/50] [Batch 700/1684] [D loss: 0.3987] [G loss: 0.5129]\n",
      "[Epoch 38/50] [Batch 800/1684] [D loss: 0.4494] [G loss: 0.5731]\n",
      "[Epoch 38/50] [Batch 900/1684] [D loss: 0.4608] [G loss: 0.5618]\n",
      "[Epoch 38/50] [Batch 1000/1684] [D loss: 0.3795] [G loss: 0.5075]\n",
      "[Epoch 38/50] [Batch 1100/1684] [D loss: 0.4783] [G loss: 0.5088]\n",
      "[Epoch 38/50] [Batch 1200/1684] [D loss: 0.3846] [G loss: 0.5290]\n",
      "[Epoch 38/50] [Batch 1300/1684] [D loss: 0.4486] [G loss: 0.5232]\n",
      "[Epoch 38/50] [Batch 1400/1684] [D loss: 0.3892] [G loss: 0.4821]\n",
      "[Epoch 38/50] [Batch 1500/1684] [D loss: 0.5144] [G loss: 0.5313]\n",
      "[Epoch 38/50] [Batch 1600/1684] [D loss: 0.4041] [G loss: 0.5878]\n",
      "[Epoch 39/50] [Batch 0/1684] [D loss: 0.4602] [G loss: 0.5275]\n",
      "[Epoch 39/50] [Batch 100/1684] [D loss: 0.4938] [G loss: 0.5151]\n",
      "[Epoch 39/50] [Batch 200/1684] [D loss: 0.5009] [G loss: 0.5192]\n",
      "[Epoch 39/50] [Batch 300/1684] [D loss: 0.5242] [G loss: 0.5456]\n",
      "[Epoch 39/50] [Batch 400/1684] [D loss: 0.5446] [G loss: 0.4991]\n",
      "[Epoch 39/50] [Batch 500/1684] [D loss: 0.4456] [G loss: 0.5186]\n",
      "[Epoch 39/50] [Batch 600/1684] [D loss: 0.4515] [G loss: 0.5336]\n",
      "[Epoch 39/50] [Batch 700/1684] [D loss: 0.5220] [G loss: 0.5368]\n",
      "[Epoch 39/50] [Batch 800/1684] [D loss: 0.5147] [G loss: 0.5216]\n",
      "[Epoch 39/50] [Batch 900/1684] [D loss: 0.5075] [G loss: 0.5840]\n",
      "[Epoch 39/50] [Batch 1000/1684] [D loss: 0.4208] [G loss: 0.5581]\n",
      "[Epoch 39/50] [Batch 1100/1684] [D loss: 0.4214] [G loss: 0.5231]\n",
      "[Epoch 39/50] [Batch 1200/1684] [D loss: 0.4654] [G loss: 0.5292]\n",
      "[Epoch 39/50] [Batch 1300/1684] [D loss: 0.5360] [G loss: 0.5501]\n",
      "[Epoch 39/50] [Batch 1400/1684] [D loss: 0.4628] [G loss: 0.5241]\n",
      "[Epoch 39/50] [Batch 1500/1684] [D loss: 0.4129] [G loss: 0.5489]\n",
      "[Epoch 39/50] [Batch 1600/1684] [D loss: 0.4945] [G loss: 0.5791]\n",
      "[Epoch 40/50] [Batch 0/1684] [D loss: 0.4413] [G loss: 0.5524]\n",
      "[Epoch 40/50] [Batch 100/1684] [D loss: 0.5166] [G loss: 0.4838]\n",
      "[Epoch 40/50] [Batch 200/1684] [D loss: 0.4320] [G loss: 0.5650]\n",
      "[Epoch 40/50] [Batch 300/1684] [D loss: 0.5124] [G loss: 0.5302]\n",
      "[Epoch 40/50] [Batch 400/1684] [D loss: 0.4314] [G loss: 0.5296]\n",
      "[Epoch 40/50] [Batch 500/1684] [D loss: 0.4725] [G loss: 0.4608]\n",
      "[Epoch 40/50] [Batch 600/1684] [D loss: 0.5007] [G loss: 0.5202]\n",
      "[Epoch 40/50] [Batch 700/1684] [D loss: 0.4188] [G loss: 0.5286]\n",
      "[Epoch 40/50] [Batch 800/1684] [D loss: 0.3972] [G loss: 0.5485]\n",
      "[Epoch 40/50] [Batch 900/1684] [D loss: 0.4890] [G loss: 0.5530]\n",
      "[Epoch 40/50] [Batch 1000/1684] [D loss: 0.4064] [G loss: 0.5451]\n",
      "[Epoch 40/50] [Batch 1100/1684] [D loss: 0.4028] [G loss: 0.5121]\n",
      "[Epoch 40/50] [Batch 1200/1684] [D loss: 0.4765] [G loss: 0.5551]\n",
      "[Epoch 40/50] [Batch 1300/1684] [D loss: 0.4939] [G loss: 0.5168]\n",
      "[Epoch 40/50] [Batch 1400/1684] [D loss: 0.4201] [G loss: 0.5174]\n",
      "[Epoch 40/50] [Batch 1500/1684] [D loss: 0.4819] [G loss: 0.5679]\n",
      "[Epoch 40/50] [Batch 1600/1684] [D loss: 0.4901] [G loss: 0.5213]\n",
      "[Epoch 41/50] [Batch 0/1684] [D loss: 0.4468] [G loss: 0.5389]\n",
      "[Epoch 41/50] [Batch 100/1684] [D loss: 0.4981] [G loss: 0.4858]\n",
      "[Epoch 41/50] [Batch 200/1684] [D loss: 0.4629] [G loss: 0.5179]\n",
      "[Epoch 41/50] [Batch 300/1684] [D loss: 0.4710] [G loss: 0.5559]\n",
      "[Epoch 41/50] [Batch 400/1684] [D loss: 0.5421] [G loss: 0.5078]\n",
      "[Epoch 41/50] [Batch 500/1684] [D loss: 0.4429] [G loss: 0.5224]\n",
      "[Epoch 41/50] [Batch 600/1684] [D loss: 0.4731] [G loss: 0.5386]\n",
      "[Epoch 41/50] [Batch 700/1684] [D loss: 0.4393] [G loss: 0.5270]\n",
      "[Epoch 41/50] [Batch 800/1684] [D loss: 0.4539] [G loss: 0.4763]\n",
      "[Epoch 41/50] [Batch 900/1684] [D loss: 0.3620] [G loss: 0.4918]\n",
      "[Epoch 41/50] [Batch 1000/1684] [D loss: 0.4256] [G loss: 0.5255]\n",
      "[Epoch 41/50] [Batch 1100/1684] [D loss: 0.4663] [G loss: 0.5223]\n",
      "[Epoch 41/50] [Batch 1200/1684] [D loss: 0.4550] [G loss: 0.5245]\n",
      "[Epoch 41/50] [Batch 1300/1684] [D loss: 0.4721] [G loss: 0.4903]\n",
      "[Epoch 41/50] [Batch 1400/1684] [D loss: 0.5847] [G loss: 0.5471]\n",
      "[Epoch 41/50] [Batch 1500/1684] [D loss: 0.4684] [G loss: 0.4939]\n",
      "[Epoch 41/50] [Batch 1600/1684] [D loss: 0.5551] [G loss: 0.5475]\n",
      "[Epoch 42/50] [Batch 0/1684] [D loss: 0.4891] [G loss: 0.4773]\n",
      "[Epoch 42/50] [Batch 100/1684] [D loss: 0.4737] [G loss: 0.5166]\n",
      "[Epoch 42/50] [Batch 200/1684] [D loss: 0.6094] [G loss: 0.5479]\n",
      "[Epoch 42/50] [Batch 300/1684] [D loss: 0.4290] [G loss: 0.5693]\n",
      "[Epoch 42/50] [Batch 400/1684] [D loss: 0.4756] [G loss: 0.4950]\n",
      "[Epoch 42/50] [Batch 500/1684] [D loss: 0.4463] [G loss: 0.5499]\n",
      "[Epoch 42/50] [Batch 600/1684] [D loss: 0.4899] [G loss: 0.5342]\n",
      "[Epoch 42/50] [Batch 700/1684] [D loss: 0.4450] [G loss: 0.5172]\n",
      "[Epoch 42/50] [Batch 800/1684] [D loss: 0.4428] [G loss: 0.5107]\n",
      "[Epoch 42/50] [Batch 900/1684] [D loss: 0.4409] [G loss: 0.5622]\n",
      "[Epoch 42/50] [Batch 1000/1684] [D loss: 0.4749] [G loss: 0.4673]\n",
      "[Epoch 42/50] [Batch 1100/1684] [D loss: 0.3984] [G loss: 0.5010]\n",
      "[Epoch 42/50] [Batch 1200/1684] [D loss: 0.4608] [G loss: 0.5223]\n",
      "[Epoch 42/50] [Batch 1300/1684] [D loss: 0.5136] [G loss: 0.5166]\n",
      "[Epoch 42/50] [Batch 1400/1684] [D loss: 0.4659] [G loss: 0.4757]\n",
      "[Epoch 42/50] [Batch 1500/1684] [D loss: 0.5024] [G loss: 0.5188]\n",
      "[Epoch 42/50] [Batch 1600/1684] [D loss: 0.4619] [G loss: 0.5537]\n",
      "[Epoch 43/50] [Batch 0/1684] [D loss: 0.5412] [G loss: 0.4405]\n",
      "[Epoch 43/50] [Batch 100/1684] [D loss: 0.4221] [G loss: 0.5556]\n",
      "[Epoch 43/50] [Batch 200/1684] [D loss: 0.4293] [G loss: 0.5100]\n",
      "[Epoch 43/50] [Batch 300/1684] [D loss: 0.5089] [G loss: 0.5437]\n",
      "[Epoch 43/50] [Batch 400/1684] [D loss: 0.4596] [G loss: 0.5331]\n",
      "[Epoch 43/50] [Batch 500/1684] [D loss: 0.5959] [G loss: 0.5386]\n",
      "[Epoch 43/50] [Batch 600/1684] [D loss: 0.4439] [G loss: 0.5348]\n",
      "[Epoch 43/50] [Batch 700/1684] [D loss: 0.4718] [G loss: 0.5641]\n",
      "[Epoch 43/50] [Batch 800/1684] [D loss: 0.4540] [G loss: 0.4959]\n",
      "[Epoch 43/50] [Batch 900/1684] [D loss: 0.4710] [G loss: 0.5797]\n",
      "[Epoch 43/50] [Batch 1000/1684] [D loss: 0.3986] [G loss: 0.4490]\n",
      "[Epoch 43/50] [Batch 1100/1684] [D loss: 0.5559] [G loss: 0.5176]\n",
      "[Epoch 43/50] [Batch 1200/1684] [D loss: 0.4994] [G loss: 0.5172]\n",
      "[Epoch 43/50] [Batch 1300/1684] [D loss: 0.4970] [G loss: 0.5241]\n",
      "[Epoch 43/50] [Batch 1400/1684] [D loss: 0.5215] [G loss: 0.4671]\n",
      "[Epoch 43/50] [Batch 1500/1684] [D loss: 0.4441] [G loss: 0.5069]\n",
      "[Epoch 43/50] [Batch 1600/1684] [D loss: 0.5585] [G loss: 0.4775]\n",
      "[Epoch 44/50] [Batch 0/1684] [D loss: 0.4763] [G loss: 0.5139]\n",
      "[Epoch 44/50] [Batch 100/1684] [D loss: 0.4191] [G loss: 0.5434]\n",
      "[Epoch 44/50] [Batch 200/1684] [D loss: 0.4934] [G loss: 0.4844]\n",
      "[Epoch 44/50] [Batch 300/1684] [D loss: 0.4401] [G loss: 0.4997]\n",
      "[Epoch 44/50] [Batch 400/1684] [D loss: 0.5058] [G loss: 0.4849]\n",
      "[Epoch 44/50] [Batch 500/1684] [D loss: 0.4307] [G loss: 0.5674]\n",
      "[Epoch 44/50] [Batch 600/1684] [D loss: 0.5459] [G loss: 0.5102]\n",
      "[Epoch 44/50] [Batch 700/1684] [D loss: 0.5019] [G loss: 0.5146]\n",
      "[Epoch 44/50] [Batch 800/1684] [D loss: 0.3901] [G loss: 0.5060]\n",
      "[Epoch 44/50] [Batch 900/1684] [D loss: 0.4542] [G loss: 0.4956]\n",
      "[Epoch 44/50] [Batch 1000/1684] [D loss: 0.4533] [G loss: 0.5415]\n",
      "[Epoch 44/50] [Batch 1100/1684] [D loss: 0.4756] [G loss: 0.5426]\n",
      "[Epoch 44/50] [Batch 1200/1684] [D loss: 0.4549] [G loss: 0.5179]\n",
      "[Epoch 44/50] [Batch 1300/1684] [D loss: 0.4679] [G loss: 0.4553]\n",
      "[Epoch 44/50] [Batch 1400/1684] [D loss: 0.4619] [G loss: 0.5051]\n",
      "[Epoch 44/50] [Batch 1500/1684] [D loss: 0.5026] [G loss: 0.5302]\n",
      "[Epoch 44/50] [Batch 1600/1684] [D loss: 0.4682] [G loss: 0.5358]\n",
      "[Epoch 45/50] [Batch 0/1684] [D loss: 0.4416] [G loss: 0.5504]\n",
      "[Epoch 45/50] [Batch 100/1684] [D loss: 0.5074] [G loss: 0.5753]\n",
      "[Epoch 45/50] [Batch 200/1684] [D loss: 0.4668] [G loss: 0.4330]\n",
      "[Epoch 45/50] [Batch 300/1684] [D loss: 0.4698] [G loss: 0.5161]\n",
      "[Epoch 45/50] [Batch 400/1684] [D loss: 0.4258] [G loss: 0.5441]\n",
      "[Epoch 45/50] [Batch 500/1684] [D loss: 0.5158] [G loss: 0.4784]\n",
      "[Epoch 45/50] [Batch 600/1684] [D loss: 0.5413] [G loss: 0.4981]\n",
      "[Epoch 45/50] [Batch 700/1684] [D loss: 0.4645] [G loss: 0.5252]\n",
      "[Epoch 45/50] [Batch 800/1684] [D loss: 0.4843] [G loss: 0.4945]\n",
      "[Epoch 45/50] [Batch 900/1684] [D loss: 0.4790] [G loss: 0.5389]\n",
      "[Epoch 45/50] [Batch 1000/1684] [D loss: 0.4728] [G loss: 0.4821]\n",
      "[Epoch 45/50] [Batch 1100/1684] [D loss: 0.4984] [G loss: 0.5393]\n",
      "[Epoch 45/50] [Batch 1200/1684] [D loss: 0.4894] [G loss: 0.5470]\n",
      "[Epoch 45/50] [Batch 1300/1684] [D loss: 0.4224] [G loss: 0.4698]\n",
      "[Epoch 45/50] [Batch 1400/1684] [D loss: 0.4599] [G loss: 0.5100]\n",
      "[Epoch 45/50] [Batch 1500/1684] [D loss: 0.5036] [G loss: 0.5382]\n",
      "[Epoch 45/50] [Batch 1600/1684] [D loss: 0.4683] [G loss: 0.5032]\n",
      "[Epoch 46/50] [Batch 0/1684] [D loss: 0.4501] [G loss: 0.5186]\n",
      "[Epoch 46/50] [Batch 100/1684] [D loss: 0.5220] [G loss: 0.5118]\n",
      "[Epoch 46/50] [Batch 200/1684] [D loss: 0.5217] [G loss: 0.4762]\n",
      "[Epoch 46/50] [Batch 300/1684] [D loss: 0.4497] [G loss: 0.5363]\n",
      "[Epoch 46/50] [Batch 400/1684] [D loss: 0.4674] [G loss: 0.5175]\n",
      "[Epoch 46/50] [Batch 500/1684] [D loss: 0.4876] [G loss: 0.5253]\n",
      "[Epoch 46/50] [Batch 600/1684] [D loss: 0.5075] [G loss: 0.4680]\n",
      "[Epoch 46/50] [Batch 700/1684] [D loss: 0.4326] [G loss: 0.5479]\n",
      "[Epoch 46/50] [Batch 800/1684] [D loss: 0.4913] [G loss: 0.4949]\n",
      "[Epoch 46/50] [Batch 900/1684] [D loss: 0.5096] [G loss: 0.5562]\n",
      "[Epoch 46/50] [Batch 1000/1684] [D loss: 0.4335] [G loss: 0.5382]\n",
      "[Epoch 46/50] [Batch 1100/1684] [D loss: 0.5349] [G loss: 0.5333]\n",
      "[Epoch 46/50] [Batch 1200/1684] [D loss: 0.5269] [G loss: 0.4734]\n",
      "[Epoch 46/50] [Batch 1300/1684] [D loss: 0.4215] [G loss: 0.5605]\n",
      "[Epoch 46/50] [Batch 1400/1684] [D loss: 0.4819] [G loss: 0.5001]\n",
      "[Epoch 46/50] [Batch 1500/1684] [D loss: 0.4291] [G loss: 0.4922]\n",
      "[Epoch 46/50] [Batch 1600/1684] [D loss: 0.5035] [G loss: 0.4947]\n",
      "[Epoch 47/50] [Batch 0/1684] [D loss: 0.4764] [G loss: 0.5561]\n",
      "[Epoch 47/50] [Batch 100/1684] [D loss: 0.4509] [G loss: 0.5492]\n",
      "[Epoch 47/50] [Batch 200/1684] [D loss: 0.4491] [G loss: 0.4608]\n",
      "[Epoch 47/50] [Batch 300/1684] [D loss: 0.4868] [G loss: 0.5321]\n",
      "[Epoch 47/50] [Batch 400/1684] [D loss: 0.4498] [G loss: 0.5263]\n",
      "[Epoch 47/50] [Batch 500/1684] [D loss: 0.4556] [G loss: 0.5043]\n",
      "[Epoch 47/50] [Batch 600/1684] [D loss: 0.4511] [G loss: 0.5076]\n",
      "[Epoch 47/50] [Batch 700/1684] [D loss: 0.4619] [G loss: 0.5149]\n",
      "[Epoch 47/50] [Batch 800/1684] [D loss: 0.4569] [G loss: 0.4265]\n",
      "[Epoch 47/50] [Batch 900/1684] [D loss: 0.4884] [G loss: 0.5352]\n",
      "[Epoch 47/50] [Batch 1000/1684] [D loss: 0.4833] [G loss: 0.5658]\n",
      "[Epoch 47/50] [Batch 1100/1684] [D loss: 0.4796] [G loss: 0.5545]\n",
      "[Epoch 47/50] [Batch 1200/1684] [D loss: 0.5211] [G loss: 0.4767]\n",
      "[Epoch 47/50] [Batch 1300/1684] [D loss: 0.4468] [G loss: 0.5230]\n",
      "[Epoch 47/50] [Batch 1400/1684] [D loss: 0.4369] [G loss: 0.5456]\n",
      "[Epoch 47/50] [Batch 1500/1684] [D loss: 0.4765] [G loss: 0.4890]\n",
      "[Epoch 47/50] [Batch 1600/1684] [D loss: 0.5288] [G loss: 0.5295]\n",
      "[Epoch 48/50] [Batch 0/1684] [D loss: 0.4275] [G loss: 0.4876]\n",
      "[Epoch 48/50] [Batch 100/1684] [D loss: 0.4309] [G loss: 0.4837]\n",
      "[Epoch 48/50] [Batch 200/1684] [D loss: 0.4564] [G loss: 0.5416]\n",
      "[Epoch 48/50] [Batch 300/1684] [D loss: 0.4374] [G loss: 0.5211]\n",
      "[Epoch 48/50] [Batch 400/1684] [D loss: 0.4933] [G loss: 0.4942]\n",
      "[Epoch 48/50] [Batch 500/1684] [D loss: 0.4843] [G loss: 0.4875]\n",
      "[Epoch 48/50] [Batch 600/1684] [D loss: 0.5121] [G loss: 0.5361]\n",
      "[Epoch 48/50] [Batch 700/1684] [D loss: 0.6159] [G loss: 0.4902]\n",
      "[Epoch 48/50] [Batch 800/1684] [D loss: 0.5029] [G loss: 0.5028]\n",
      "[Epoch 48/50] [Batch 900/1684] [D loss: 0.4434] [G loss: 0.5797]\n",
      "[Epoch 48/50] [Batch 1000/1684] [D loss: 0.4810] [G loss: 0.5422]\n",
      "[Epoch 48/50] [Batch 1100/1684] [D loss: 0.4981] [G loss: 0.4950]\n",
      "[Epoch 48/50] [Batch 1200/1684] [D loss: 0.4339] [G loss: 0.4708]\n",
      "[Epoch 48/50] [Batch 1300/1684] [D loss: 0.4390] [G loss: 0.5088]\n",
      "[Epoch 48/50] [Batch 1400/1684] [D loss: 0.5005] [G loss: 0.5067]\n",
      "[Epoch 48/50] [Batch 1500/1684] [D loss: 0.4090] [G loss: 0.5506]\n",
      "[Epoch 48/50] [Batch 1600/1684] [D loss: 0.4695] [G loss: 0.4963]\n",
      "[Epoch 49/50] [Batch 0/1684] [D loss: 0.5303] [G loss: 0.4834]\n",
      "[Epoch 49/50] [Batch 100/1684] [D loss: 0.4698] [G loss: 0.4348]\n",
      "[Epoch 49/50] [Batch 200/1684] [D loss: 0.4674] [G loss: 0.5185]\n",
      "[Epoch 49/50] [Batch 300/1684] [D loss: 0.4699] [G loss: 0.5127]\n",
      "[Epoch 49/50] [Batch 400/1684] [D loss: 0.4919] [G loss: 0.4628]\n",
      "[Epoch 49/50] [Batch 500/1684] [D loss: 0.4916] [G loss: 0.4935]\n",
      "[Epoch 49/50] [Batch 600/1684] [D loss: 0.5292] [G loss: 0.4637]\n",
      "[Epoch 49/50] [Batch 700/1684] [D loss: 0.4054] [G loss: 0.5084]\n",
      "[Epoch 49/50] [Batch 800/1684] [D loss: 0.4313] [G loss: 0.5584]\n",
      "[Epoch 49/50] [Batch 900/1684] [D loss: 0.4953] [G loss: 0.5298]\n",
      "[Epoch 49/50] [Batch 1000/1684] [D loss: 0.4986] [G loss: 0.4476]\n",
      "[Epoch 49/50] [Batch 1100/1684] [D loss: 0.5461] [G loss: 0.4751]\n",
      "[Epoch 49/50] [Batch 1200/1684] [D loss: 0.5179] [G loss: 0.5335]\n",
      "[Epoch 49/50] [Batch 1300/1684] [D loss: 0.5357] [G loss: 0.4409]\n",
      "[Epoch 49/50] [Batch 1400/1684] [D loss: 0.4847] [G loss: 0.4975]\n",
      "[Epoch 49/50] [Batch 1500/1684] [D loss: 0.4248] [G loss: 0.5024]\n",
      "[Epoch 49/50] [Batch 1600/1684] [D loss: 0.4597] [G loss: 0.5498]\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    for i, imgs in enumerate(dataloader):\n",
    "        batch_size = imgs.size(0)\n",
    "        real_imgs = imgs.to(device)\n",
    "\n",
    "        noise_factor = max(0.1 * (1 - epoch/20), 0)  # Starts at 0.1, goes to 0 by epoch 20\n",
    "\n",
    "        # Train Discriminator\n",
    "        for _ in range(n_critic):\n",
    "            optimizer_D.zero_grad()\n",
    "            \n",
    "            # Generate fake images\n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            fake_imgs = generator(z)\n",
    "            \n",
    "            # Calculate discriminator outputs\n",
    "            real_imgs_noisy = real_imgs + noise_factor * torch.randn_like(real_imgs)\n",
    "            fake_imgs_noisy = fake_imgs.detach() + noise_factor * torch.randn_like(fake_imgs)\n",
    "            real_validity = discriminator(real_imgs_noisy)\n",
    "            fake_validity = discriminator(fake_imgs_noisy)\n",
    "            \n",
    "            # Calculate gradient penalty\n",
    "            gradient_penalty = compute_gradient_penalty(discriminator, real_imgs_noisy, fake_imgs_noisy, device)\n",
    "\n",
    "            real_target = torch.ones_like(real_validity).to(device) * 0.9  # Smooth positive labels\n",
    "            fake_target = torch.ones_like(fake_validity).to(device) * -0.9  # Smooth negative labels\n",
    "            d_real_loss = torch.mean(nn.MSELoss()(real_validity, real_target))\n",
    "            d_fake_loss = torch.mean(nn.MSELoss()(fake_validity, fake_target))\n",
    "            d_loss = d_real_loss + d_fake_loss + lambda_gp * gradient_penalty            \n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "            \n",
    "            # Remove weight clamping - this shouldn't be used with gradient penalty\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # Generate new fake images (important to regenerate after D update)\n",
    "        z = torch.randn(batch_size, latent_dim).to(device)\n",
    "        fake_imgs = generator(z)\n",
    "        \n",
    "        fake_validity = discriminator(fake_imgs)\n",
    "        g_loss = -torch.mean(fake_validity)\n",
    "        \n",
    "        g_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n",
    "        optimizer_G.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(dataloader)}] \"\n",
    "                  f\"[D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}]\")\n",
    "            \n",
    "\n",
    "    # Save generated images\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            fake_imgs = generator(torch.randn(16, latent_dim).to(device))\n",
    "            fake_imgs = fake_imgs.cpu().numpy()\n",
    "            fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
    "            for i, ax in enumerate(axes.flatten()):\n",
    "                ax.imshow(fake_imgs[i, 0], cmap='gray')\n",
    "                ax.axis('off')\n",
    "            plt.savefig(f'generated_digits_epoch_{epoch}.png')\n",
    "            plt.close()\n",
    "\n",
    "    scheduler_G.step()\n",
    "    scheduler_D.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3116ade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "torch.save(generator.state_dict(), 'generator.pth')\n",
    "torch.save(discriminator.state_dict(), 'discriminator.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d4f937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
